[{"text": "Henry: \u201cWill Artificial Intelligence ever\nreplace humans?\u201d is a hotly debated question", "start": 1.41, "duration": 4.119}, {"text": "these days.", "start": 5.529, "duration": 1.0}, {"text": "Some people claim computers will eventually\ngain superintelligence, be able to outperform", "start": 6.529, "duration": 3.571}, {"text": "humans on any task, and destroy humanity.", "start": 10.1, "duration": 2.8}, {"text": "Other people say \u201cdon\u2019t worry, AI will\njust be another tool we can use and control,", "start": 12.9, "duration": 3.399}, {"text": "like our current computers.\u201d", "start": 16.299, "duration": 1.301}, {"text": "So we\u2019ve got physicist and AI-researcher\nMax Tegmark back again to share with us the", "start": 17.6, "duration": 4.019}, {"text": "collective takeaways from the recent Asilomar\nconference on the future of AI that he helped", "start": 21.619, "duration": 3.921}, {"text": "organize \u2013\u00a0and he\u2019s going to help separate\nAI myths from AI facts.", "start": 25.54, "duration": 3.63}, {"text": "Max: Hello!", "start": 29.17, "duration": 1.0}, {"text": "Henry: First of all, Max, machines (including\ncomputers) have long been better than us at", "start": 30.17, "duration": 3.88}, {"text": "many tasks, like arithmetic, or weaving, but\nthose are often repetitive and mechanical", "start": 34.05, "duration": 4.12}, {"text": "operations.", "start": 38.17, "duration": 1.0}, {"text": "So why shouldn\u2019t I believe that there are\nsome things that are simply impossible for", "start": 39.17, "duration": 3.069}, {"text": "machines to do as well as people?", "start": 42.239, "duration": 1.701}, {"text": "Say making minutephysics videos, or consoling\na friend?", "start": 43.94, "duration": 2.57}, {"text": "Max: Well, we\u2019ve traditionally thought of\nintelligence as something mysterious that", "start": 46.51, "duration": 3.639}, {"text": "can only exist in biological organisms, especially\nhumans.", "start": 50.149, "duration": 3.221}, {"text": "But from the perspective of modern physical\nscience, intelligence is simply a particular", "start": 53.37, "duration": 3.91}, {"text": "kind of information processing and reacting\nperformed by particular arrangements of elementary", "start": 57.28, "duration": 4.329}, {"text": "particles moving around, and there\u2019s no\nlaw of physics that says it\u2019s impossible", "start": 61.609, "duration": 3.63}, {"text": "to do that kind of information processing\nbetter than humans already do.", "start": 65.239, "duration": 3.231}, {"text": "It\u2019s not a stretch to say that earthworms\nprocess information better than rocks, and", "start": 68.47, "duration": 3.89}, {"text": "humans better than earthworms, and in many\nareas, machines are already better than humans.", "start": 72.36, "duration": 3.94}, {"text": "This suggests we\u2019ve likely only seen the\ntip of the intelligence iceberg, and that", "start": 76.3, "duration": 3.58}, {"text": "we\u2019re on track to unlock the full intelligence\nthat\u2019s latent in nature and use it to help", "start": 79.88, "duration": 4.13}, {"text": "humanity flourish - or flounder.", "start": 84.01, "duration": 2.18}, {"text": "Henry: So how do we keep ourselves on the\nright side of the \u201cflourish or flounder\u201d", "start": 86.19, "duration": 4.27}, {"text": "balance?", "start": 90.46, "duration": 1.0}, {"text": "What, if anything, should we really be concerned\nabout with superintelligent AI?", "start": 91.46, "duration": 3.08}, {"text": "Max: Here\u2019s what has many top AI researchers\nconcerned: not machines or computers turning", "start": 94.54, "duration": 4.84}, {"text": "evil, but something more subtle: superintelligence\nthat simply doesn\u2019t share our goals.", "start": 99.38, "duration": 4.21}, {"text": "\u00a0If a heat-seeking missile is homing in on\nyou, you probably wouldn\u2019t think: \u201cNo", "start": 103.59, "duration": 3.83}, {"text": "need to worry, it\u2019s not evil, it\u2019s just\nfollowing its programming.\u201d", "start": 107.42, "duration": 2.88}, {"text": "No, what matters to you is what the heat-seeking\nmissile does and how well it does it, not", "start": 110.3, "duration": 5.15}, {"text": "what it\u2019s feeling, or whether it has feelings\nat all.", "start": 115.45, "duration": 2.14}, {"text": "The real worry isn\u2019t malevolence, but competence.", "start": 117.59, "duration": 3.27}, {"text": "A superintelligent AI is by definition very\ngood at attaining its goals, so the most important", "start": 120.86, "duration": 4.66}, {"text": "thing for us to do is to ensure that its goals\nare aligned with ours.", "start": 125.52, "duration": 3.54}, {"text": "As an analogy, humans are more intelligent\nand competent than ants, \u00a0and if we want", "start": 129.06, "duration": 4.27}, {"text": "to build a hydroelectric dam where there happens\nto be an anthill, there may no malevolence", "start": 133.33, "duration": 3.84}, {"text": "involved, but, well... too bad for the ants.", "start": 137.17, "duration": 2.83}, {"text": "Cats and dogs, on the other hand, have done\na great job of aligning their goals with the", "start": 140.0, "duration": 3.75}, {"text": "goals of humans \u2013 I mean, even though I\u2019m\na physicist, I can\u2019t help think kittens", "start": 143.75, "duration": 4.26}, {"text": "are the cutest particle arrangements in our\nuniverse...", "start": 148.01, "duration": 3.1}, {"text": "If we build superintelligence, we\u2019d be better\noff in the position of cats and dogs than", "start": 151.11, "duration": 3.96}, {"text": "ants.", "start": 155.07, "duration": 1.0}, {"text": "Or better yet, we\u2019ll figure out how to ensure\nthat AI adopts our goals rather than the other", "start": 156.07, "duration": 3.99}, {"text": "way around.", "start": 160.06, "duration": 1.0}, {"text": "Henry: And when exactly is superintelligence\ngoing to arrive?", "start": 161.06, "duration": 2.87}, {"text": "When do we need to start panicking?", "start": 163.93, "duration": 1.34}, {"text": "Max: First of all, Henry, superintelligence\ndoesn\u2019t have to be something negative.", "start": 165.27, "duration": 3.43}, {"text": "In fact, if we get it right, AI might become\nthe best thing ever to happen to humanity.", "start": 168.7, "duration": 4.33}, {"text": "Everything I love about civilization is the\nproduct of intelligence, so if AI amplifies", "start": 173.03, "duration": 4.71}, {"text": "our collective intelligence enough to solve\ntoday\u2019s and tomorrow\u2019s greatest problems,", "start": 177.74, "duration": 3.95}, {"text": "humanity might flourish like never before.", "start": 181.69, "duration": 2.33}, {"text": "Second, most AI researchers think superintelligence\nis at least decades away...", "start": 184.02, "duration": 4.75}, {"text": "Buuuut the research needed to ensure that\nit remains beneficial to humanity (rather", "start": 188.77, "duration": 3.18}, {"text": "than harmful) might also take decades, so\nwe need to start right away.", "start": 191.95, "duration": 4.32}, {"text": "For example, we\u2019ll need to figure out how\nto ensure machines learn the collective goals", "start": 196.27, "duration": 3.44}, {"text": "of humanity, adopt these goals for themselves,\nand retain the goals as they keep getting", "start": 199.71, "duration": 4.57}, {"text": "smarter.", "start": 204.28, "duration": 1.0}, {"text": "And what about when our goals disagree?", "start": 205.28, "duration": 1.31}, {"text": "Should we vote on what the machine\u2019s goals\nshould be?", "start": 206.59, "duration": 2.37}, {"text": "Just do whatever the president wants?", "start": 208.96, "duration": 2.34}, {"text": "Whatever the creator of the superintelligence\nwants?", "start": 211.3, "duration": 2.31}, {"text": "Let the AI decide?", "start": 213.61, "duration": 1.15}, {"text": "\u00a0In a very real way, the question of how\nto live with superintelligence is a question", "start": 214.76, "duration": 4.11}, {"text": "of what sort of future we want to create for\nhumanity.", "start": 218.87, "duration": 2.78}, {"text": "Which obviously shouldn\u2019t just be left to\nAI researchers, as caring and socially skilled", "start": 221.65, "duration": 4.41}, {"text": "as we are.;)", "start": 226.06, "duration": 1.0}, {"text": "Henry: Thanks, Max!", "start": 227.06, "duration": 1.0}, {"text": "So, uh, how do I get involved to make sure\nwe don\u2019t end up living in a superintelligence-powered", "start": 228.06, "duration": 4.44}, {"text": "dictatorship?", "start": 232.5, "duration": 1.0}, {"text": "Max: At the Future of Life Institute (Henry\ninterjects: which is sponsoring this video),", "start": 233.5, "duration": 2.47}, {"text": "we\u2019ve built a site where you can go to answer\nquestions, ask questions, and otherwise contribute", "start": 235.97, "duration": 4.31}, {"text": "your thoughts to help shape the future of\nAI policy and research.", "start": 240.28, "duration": 2.882}, {"text": "Link\u2019s in the video description.", "start": 243.162, "duration": 2.418}, {"text": "Henry: Awesome.", "start": 245.58, "duration": 1.31}]