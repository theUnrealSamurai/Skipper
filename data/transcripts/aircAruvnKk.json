[{"text": "This is a three. It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels.", "start": 4.02, "duration": 6.66}, {"text": "But your brain has no trouble recognizing it as a three and I want you to take a moment to appreciate", "start": 10.68, "duration": 4.98}, {"text": "How crazy it is that brains can do this so effortlessly?", "start": 15.9, "duration": 3.049}, {"text": "I mean this this and this are also recognizable as threes,", "start": 18.949, "duration": 4.211}, {"text": "even though the specific values of each pixel is very different from one image to the next.", "start": 23.16, "duration": 4.9}, {"text": "The particular light-sensitive cells in your eye that are firing when you see this three", "start": 28.08, "duration": 5.7}, {"text": "are very different from the ones firing when you see this three.", "start": 33.78, "duration": 3.02}, {"text": "But something in that crazy smart visual cortex of yours", "start": 37.14, "duration": 3.47}, {"text": "resolves these as representing the same idea while at the same time recognizing other images as their own distinct ideas", "start": 41.129, "duration": 7.01}, {"text": "But if I told you hey sit down and write for me a program that takes in a grid of 28 by 28", "start": 48.84, "duration": 6.199}, {"text": "pixels like this and outputs a single number between 0 and 10 telling you what it thinks the digit is", "start": 55.379, "duration": 6.38}, {"text": "Well the task goes from comically trivial to dauntingly difficult", "start": 62.25, "duration": 3.889}, {"text": "Unless you've been living under a rock", "start": 66.75, "duration": 1.52}, {"text": "I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present into the future", "start": 68.27, "duration": 6.329}, {"text": "But what I want to do here is show you what a neural network actually is", "start": 74.64, "duration": 3.77}, {"text": "Assuming no background and to help visualize what it's doing not as a buzzword but as a piece of math", "start": 78.66, "duration": 5.569}, {"text": "My hope is just that you come away feeling like this structure itself is", "start": 84.57, "duration": 3.74}, {"text": "Motivated and to feel like you know what it means when you read or you hear about a neural network quote-unquote learning", "start": 88.38, "duration": 6.019}, {"text": "This video is just going to be devoted to the structure component of that and the following one is going to tackle learning", "start": 94.95, "duration": 5.299}, {"text": "What we're going to do is put together a neural network that can learn to recognize handwritten digits", "start": 100.53, "duration": 5.42}, {"text": "This is a somewhat classic example for", "start": 109.27, "duration": 2.059}, {"text": "Introducing the topic and I'm happy to stick with the status quo here because at the end of the two videos I want to point", "start": 111.52, "duration": 5.239}, {"text": "You to a couple good resources where you can learn more and where you can download the code that does this and play with it?", "start": 116.76, "duration": 5.339}, {"text": "on your own computer", "start": 122.1, "duration": 2.0}, {"text": "There are many many variants of neural networks and in recent years", "start": 124.75, "duration": 4.22}, {"text": "There's been sort of a boom in research towards these variants", "start": 128.97, "duration": 3.0}, {"text": "But in these two introductory videos you and I are just going to look at the simplest plain-vanilla form with no added frills", "start": 132.13, "duration": 6.889}, {"text": "This is kind of a necessary", "start": 139.3, "duration": 1.74}, {"text": "prerequisite for understanding any of the more powerful modern variants and", "start": 141.04, "duration": 3.47}, {"text": "Trust me it still has plenty of complexity for us to wrap our minds around", "start": 144.76, "duration": 3.439}, {"text": "But even in this simplest form it can learn to recognize handwritten digits", "start": 148.69, "duration": 4.13}, {"text": "Which is a pretty cool thing for a computer to be able to do.", "start": 152.82, "duration": 3.36}, {"text": "And at the same time you'll see how it does fall short of a couple hopes that we might have for it", "start": 157.12, "duration": 4.84}, {"text": "As the name suggests neural networks are inspired by the brain, but let's break that down", "start": 163.09, "duration": 5.089}, {"text": "What are the neurons and in what sense are they linked together?", "start": 168.52, "duration": 2.869}, {"text": "Right now when I say neuron all I want you to think about is a thing that holds a number", "start": 172.09, "duration": 5.66}, {"text": "Specifically a number between 0 & 1 it's really not more than that", "start": 178.209, "duration": 3.92}, {"text": "For example the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image", "start": 183.43, "duration": 7.7}, {"text": "which is", "start": 191.4, "duration": 1.06}, {"text": "784 neurons in total each one of these holds a number that represents the grayscale value of the corresponding pixel", "start": 192.46, "duration": 7.78}, {"text": "ranging from 0 for black pixels up to 1 for white pixels", "start": 200.769, "duration": 3.53}, {"text": "This number inside the neuron is called its activation and the image you might have in mind here", "start": 204.91, "duration": 5.509}, {"text": "Is that each neuron is lit up when its activation is a high number?", "start": 210.42, "duration": 3.539}, {"text": "So all of these 784 neurons make up the first layer of our network", "start": 216.26, "duration": 5.299}, {"text": "Now jumping over to the last layer this has ten neurons each representing one of the digits", "start": 225.99, "duration": 5.299}, {"text": "the activation in these neurons again some number that's between zero and one", "start": 231.57, "duration": 4.669}, {"text": "Represents how much the system thinks that a given image?", "start": 236.88, "duration": 3.169}, {"text": "Corresponds with a given digit. There's also a couple layers in between called the hidden layers", "start": 240.72, "duration": 5.27}, {"text": "Which for the time being?", "start": 246.18, "duration": 1.59}, {"text": "Should just be a giant question mark for how on earth this process of recognizing digits is going to be handled", "start": 247.77, "duration": 5.779}, {"text": "In this network I chose two hidden layers each one with 16 neurons and admittedly that's kind of an arbitrary choice", "start": 253.74, "duration": 6.469}, {"text": "to be honest I chose two layers based on how I want to motivate the structure in just a moment and", "start": 260.609, "duration": 4.28}, {"text": "16 well that was just a nice number to fit on the screen in practice", "start": 265.35, "duration": 3.829}, {"text": "There is a lot of room for experiment with a specific structure here", "start": 269.18, "duration": 3.029}, {"text": "The way the network operates activations in one layer determine the activations of the next layer", "start": 272.73, "duration": 5.599}, {"text": "And of course the heart of the network as an information processing mechanism comes down to exactly how those", "start": 278.76, "duration": 6.589}, {"text": "activations from one layer bring about activations in the next layer", "start": 285.57, "duration": 2.839}, {"text": "It's meant to be loosely analogous to how in biological networks of neurons some groups of neurons firing", "start": 288.9, "duration": 5.959}, {"text": "cause certain others to fire", "start": 295.41, "duration": 2.0}, {"text": "Now the network", "start": 297.57, "duration": 0.77}, {"text": "I'm showing here has already been trained to recognize digits and let me show you what I mean by that", "start": 298.34, "duration": 4.679}, {"text": "It means if you feed in an image lighting up all", "start": 303.14, "duration": 3.44}, {"text": "784 neurons of the input layer according to the brightness of each pixel in the image", "start": 306.64, "duration": 5.14}, {"text": "That pattern of activations causes some very specific pattern in the next layer", "start": 312.33, "duration": 4.699}, {"text": "Which causes some pattern in the one after it?", "start": 317.19, "duration": 2.119}, {"text": "Which finally gives some pattern in the output layer and?", "start": 319.44, "duration": 2.75}, {"text": "The brightest neuron of that output layer is the network's choice so to speak for what digit this image represents?", "start": 322.35, "duration": 7.009}, {"text": "And before jumping into the math for how one layer influences the next or how training works?", "start": 332.07, "duration": 4.789}, {"text": "Let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently", "start": 337.14, "duration": 5.929}, {"text": "What are we expecting here? What is the best hope for what those middle layers might be doing?", "start": 343.8, "duration": 4.46}, {"text": "Well when you or I recognize digits we piece together various components a nine has a loop up top and a line on the right", "start": 348.86, "duration": 7.86}, {"text": "an 8 also has a loop up top, but it's paired with another loop down low", "start": 357.26, "duration": 4.02}, {"text": "A 4 basically breaks down into three specific lines and things like that", "start": 362.02, "duration": 4.579}, {"text": "Now in a perfect world we might hope that each neuron in the second-to-last layer", "start": 367.18, "duration": 4.79}, {"text": "corresponds with one of these sub components", "start": 372.64, "duration": 2.089}, {"text": "That anytime you feed in an image with say a loop up top like a 9 or an 8", "start": 374.89, "duration": 4.85}, {"text": "There's some specific", "start": 379.87, "duration": 1.35}, {"text": "Neuron whose activation is going to be close to one and I don't mean this specific loop of pixels the hope would be that any", "start": 381.22, "duration": 6.529}, {"text": "Generally loopy pattern towards the top sets off this neuron that way going from the third layer to the last one", "start": 388.09, "duration": 6.949}, {"text": "just requires learning which combination of sub components corresponds to which digits", "start": 395.38, "duration": 4.58}, {"text": "Of course that just kicks the problem down the road", "start": 400.51, "duration": 2.3}, {"text": "Because how would you recognize these sub components or even learn what the right sub components should be and I still haven't even talked about", "start": 402.91, "duration": 6.109}, {"text": "How one layer influences the next but run with me on this one for a moment", "start": 409.02, "duration": 3.809}, {"text": "recognizing a loop can also break down into subproblems", "start": 413.65, "duration": 2.69}, {"text": "One reasonable way to do this would be to first recognize the various little edges that make it up", "start": 416.86, "duration": 5.69}, {"text": "Similarly a long line like the kind you might see in the digits 1 or 4 or 7", "start": 423.52, "duration": 5.39}, {"text": "Well that's really just a long edge or maybe you think of it as a certain pattern of several smaller edges", "start": 428.91, "duration": 5.369}, {"text": "So maybe our hope is that each neuron in the second layer of the network", "start": 434.74, "duration": 4.639}, {"text": "corresponds with the various relevant little edges", "start": 440.29, "duration": 2.36}, {"text": "Maybe when an image like this one comes in it lights up all of the neurons", "start": 443.23, "duration": 5.029}, {"text": "associated with around eight to ten specific little edges", "start": 448.72, "duration": 2.929}, {"text": "which in turn lights up the neurons associated with the upper loop and a long vertical line and", "start": 451.93, "duration": 5.0}, {"text": "Those light up the neuron associated with a nine", "start": 457.3, "duration": 2.299}, {"text": "whether or not", "start": 460.3, "duration": 0.8}, {"text": "This is what our final network actually does is another question, one that I'll come back to once we see how to train the network", "start": 461.1, "duration": 5.97}, {"text": "But this is a hope that we might have. A sort of goal with the layered structure like this", "start": 467.35, "duration": 4.82}, {"text": "Moreover you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks", "start": 473.02, "duration": 6.32}, {"text": "And even beyond image recognition there are all sorts of intelligent things you might want to do that break down into layers of abstraction", "start": 479.74, "duration": 7.009}, {"text": "Parsing speech for example involves taking raw audio and picking out distinct sounds which combine to make certain syllables", "start": 487.69, "duration": 6.98}, {"text": "Which combine to form words which combine to make up phrases and more abstract thoughts etc", "start": 495.07, "duration": 4.759}, {"text": "But getting back to how any of this actually works picture yourself right now designing", "start": 500.77, "duration": 4.94}, {"text": "How exactly the activations in one layer might determine the activations in the next?", "start": 505.71, "duration": 4.739}, {"text": "The goal is to have some mechanism that could conceivably combine pixels into edges", "start": 510.67, "duration": 5.209}, {"text": "Or edges into patterns or patterns into digits and to zoom in on one very specific example", "start": 515.88, "duration": 5.55}, {"text": "Let's say the hope is for one particular", "start": 521.95, "duration": 2.239}, {"text": "Neuron in the second layer to pick up on whether or not the image has an edge in this region here", "start": 524.38, "duration": 6.05}, {"text": "The question at hand is what parameters should the network have", "start": 530.95, "duration": 4.01}, {"text": "what dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern or", "start": 535.27, "duration": 7.22}, {"text": "Any other pixel pattern or the pattern that several edges can make a loop and other such things?", "start": 542.59, "duration": 4.7}, {"text": "Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer", "start": 548.29, "duration": 7.099}, {"text": "These weights are just numbers", "start": 555.85, "duration": 2.0}, {"text": "then take all those activations from the first layer and compute their weighted sum according to these weights I", "start": 558.19, "duration": 7.4}, {"text": "Find it helpful to think of these weights as being organized into a little grid of their own", "start": 567.37, "duration": 4.31}, {"text": "And I'm going to use green pixels to indicate positive weights and red pixels to indicate negative weights", "start": 571.68, "duration": 5.399}, {"text": "Where the brightness of that pixel is some loose depiction of the weights value?", "start": 577.24, "duration": 4.43}, {"text": "Now if we made the weights associated with almost all of the pixels zero", "start": 582.4, "duration": 3.44}, {"text": "except for some positive weights in this region that we care about", "start": 586.15, "duration": 2.929}, {"text": "then taking the weighted sum of", "start": 589.48, "duration": 1.83}, {"text": "all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about", "start": 591.31, "duration": 6.38}, {"text": "And, if you really want it to pick up on whether there's an edge here what you might do is have some negative weights", "start": 598.87, "duration": 5.57}, {"text": "associated with the surrounding pixels", "start": 604.9, "duration": 2.0}, {"text": "Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker", "start": 607.03, "duration": 5.63}, {"text": "When you compute a weighted sum like this you might come out with any number", "start": 614.279, "duration": 3.89}, {"text": "but for this network what we want is for activations to be some value between 0 & 1", "start": 618.24, "duration": 4.94}, {"text": "so a common thing to do is to pump this weighted sum", "start": 623.73, "duration": 2.869}, {"text": "Into some function that squishes the real number line into the range between 0 & 1 and", "start": 626.91, "duration": 5.09}, {"text": "A common function that does this is called the sigmoid function also known as a logistic curve", "start": 632.19, "duration": 5.059}, {"text": "basically very negative inputs end up close to zero very positive inputs end up close to 1", "start": 637.98, "duration": 5.359}, {"text": "and it just steadily increases around the input 0", "start": 643.339, "duration": 3.059}, {"text": "So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is", "start": 649.08, "duration": 6.949}, {"text": "But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0", "start": 657.45, "duration": 4.369}, {"text": "Maybe you only want it to be active when the sum is bigger than say 10", "start": 662.1, "duration": 4.16}, {"text": "That is you want some bias for it to be inactive", "start": 666.63, "duration": 3.649}, {"text": "what we'll do then is just add in some other number like negative 10 to this weighted sum", "start": 670.86, "duration": 5.239}, {"text": "Before plugging it through the sigmoid squishification function", "start": 676.529, "duration": 3.14}, {"text": "That additional number is called the bias", "start": 680.22, "duration": 2.51}, {"text": "So the weights tell you what pixel pattern this neuron in the second layer is picking up on and the bias", "start": 683.31, "duration": 5.75}, {"text": "tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active", "start": 689.22, "duration": 6.23}, {"text": "And that is just one neuron", "start": 695.91, "duration": 2.0}, {"text": "Every other neuron in this layer is going to be connected to all", "start": 698.12, "duration": 3.82}, {"text": "784 pixels neurons from the first layer and each one of those 784 connections has its own weight associated with it", "start": 702.32, "duration": 8.3}, {"text": "also each one has some bias some other number that you add on to the weighted sum before squishing it with the sigmoid and", "start": 711.33, "duration": 6.409}, {"text": "That's a lot to think about with this hidden layer of 16 neurons", "start": 718.02, "duration": 3.889}, {"text": "that's a total of 784 times 16 weights along with 16 biases", "start": 722.01, "duration": 6.26}, {"text": "And all of that is just the connections from the first layer to the second the connections between the other layers", "start": 728.49, "duration": 5.539}, {"text": "Also, have a bunch of weights and biases associated with them", "start": 734.029, "duration": 3.179}, {"text": "All said and done this network has almost exactly", "start": 737.76, "duration": 2.92}, {"text": "13,000 total weights and biases", "start": 741.28, "duration": 2.64}, {"text": "13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways", "start": 744.28, "duration": 5.26}, {"text": "So when we talk about learning?", "start": 750.52, "duration": 2.0}, {"text": "What that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve", "start": 752.53, "duration": 7.669}, {"text": "the problem at hand", "start": 760.2, "duration": 1.99}, {"text": "one thought", "start": 762.19, "duration": 0.81}, {"text": "Experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand", "start": 763.0, "duration": 6.979}, {"text": "Purposefully tweaking the numbers so that the second layer picks up on edges the third layer picks up on patterns etc", "start": 770.38, "duration": 5.779}, {"text": "I personally find this satisfying rather than just reading the network as a total black box", "start": 776.35, "duration": 5.09}, {"text": "Because when the network doesn't perform the way you", "start": 781.87, "duration": 2.479}, {"text": "anticipate if you've built up a little bit of a relationship with what those weights and biases actually mean you have a starting place for", "start": 784.6, "duration": 6.77}, {"text": "Experimenting with how to change the structure to improve or when the network does work?", "start": 791.68, "duration": 4.609}, {"text": "But not for the reasons you might expect", "start": 796.29, "duration": 2.0}, {"text": "Digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible", "start": 798.31, "duration": 6.859}, {"text": "solutions", "start": 805.18, "duration": 1.17}, {"text": "By the way the actual function here is a little cumbersome to write down. Don't you think?", "start": 806.35, "duration": 4.25}, {"text": "So let me show you a more notationally compact way that these connections are represented. This is how you'd see it", "start": 812.35, "duration": 6.11}, {"text": "If you choose to read up more about neural networks", "start": 818.46, "duration": 2.0}, {"text": "Organize all of the activations from one layer into a column as a vector", "start": 821.11, "duration": 4.7}, {"text": "Then organize all of the weights as a matrix where each row of that matrix", "start": 827.47, "duration": 4.85}, {"text": "corresponds to the connections between one layer and a particular neuron in the next layer", "start": 832.9, "duration": 4.759}, {"text": "What that means is that taking the weighted sum of the activations in the first layer according to these weights?", "start": 838.06, "duration": 5.539}, {"text": "Corresponds to one of the terms in the matrix vector product of everything we have on the left here", "start": 844.0, "duration": 5.33}, {"text": "By the way so much of machine learning just comes down to having a good grasp of linear algebra", "start": 853.54, "duration": 4.84}, {"text": "So for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means take a look at the series I did on linear algebra", "start": 858.38, "duration": 8.56}, {"text": "especially chapter three", "start": 867.25, "duration": 1.589}, {"text": "Back to our expression instead of talking about adding the bias to each one of these values independently we represent it by", "start": 868.839, "duration": 6.92}, {"text": "Organizing all those biases into a vector and adding the entire vector to the previous matrix vector product", "start": 876.01, "duration": 6.199}, {"text": "Then as a final step", "start": 882.91, "duration": 1.13}, {"text": "I'll rap a sigmoid around the outside here", "start": 884.04, "duration": 3.21}, {"text": "And what that's supposed to represent is that you're going to apply the sigmoid function to each specific", "start": 887.25, "duration": 4.649}, {"text": "component of the resulting vector inside", "start": 892.42, "duration": 2.15}, {"text": "So once you write down this weight matrix and these vectors as their own symbols you can", "start": 895.51, "duration": 5.239}, {"text": "communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression and", "start": 901.0, "duration": 6.589}, {"text": "This makes the relevant code both a lot simpler and a lot faster since many libraries optimize the heck out of matrix multiplication", "start": 907.93, "duration": 7.07}, {"text": "Remember how earlier I said these neurons are simply things that hold numbers", "start": 917.56, "duration": 3.799}, {"text": "Well of course the specific numbers that they hold depends on the image you feed in", "start": 921.79, "duration": 4.46}, {"text": "So it's actually more accurate to think of each neuron as a function one that takes in the", "start": 927.79, "duration": 5.15}, {"text": "outputs of all the neurons in the previous layer and spits out a number between zero and one", "start": 933.07, "duration": 5.0}, {"text": "Really the entire network is just a function one that takes in", "start": 938.8, "duration": 3.47}, {"text": "784 numbers as an input and spits out ten numbers as an output", "start": 942.76, "duration": 4.25}, {"text": "It's an absurdly", "start": 947.47, "duration": 1.23}, {"text": "Complicated function one that involves thirteen thousand parameters in the forms of these weights and biases that pick up on certain patterns and which involves", "start": 948.7, "duration": 7.549}, {"text": "iterating many matrix vector products and the sigmoid squish evocation function", "start": 956.25, "duration": 4.02}, {"text": "But it's just a function nonetheless and in a way it's kind of reassuring that it looks complicated", "start": 960.61, "duration": 5.78}, {"text": "I mean if it were any simpler what hope would we have that it could take on the challenge of recognizing digits?", "start": 966.39, "duration": 5.849}, {"text": "And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh?", "start": 972.96, "duration": 6.599}, {"text": "That's what I'll show in the next video, and I'll also dig a little more into what this particular network we are seeing is really doing", "start": 980.08, "duration": 5.959}, {"text": "Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out", "start": 987.13, "duration": 5.51}, {"text": "But realistically most of you don't actually receive notifications from YouTube, do you ?", "start": 992.76, "duration": 4.8}, {"text": "Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's", "start": 997.56, "duration": 4.7}, {"text": "Recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you", "start": 1002.459, "duration": 5.18}, {"text": "anyway stay posted for more", "start": 1008.25, "duration": 2.0}, {"text": "Thank you very much to everyone supporting these videos on patreon", "start": 1010.41, "duration": 3.14}, {"text": "I've been a little slow to progress in the probability series this summer", "start": 1013.589, "duration": 3.17}, {"text": "But I'm jumping back into it after this project so patrons you can look out for updates there", "start": 1016.76, "duration": 4.619}, {"text": "To close things off here I have with me Lisha Li", "start": 1023.31, "duration": 2.24}, {"text": "Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called amplify partners", "start": 1025.55, "duration": 6.479}, {"text": "Who kindly provided some of the funding for this video so Lisha one thing", "start": 1032.03, "duration": 4.5}, {"text": "I think we should quickly bring up is this sigmoid function", "start": 1036.53, "duration": 2.579}, {"text": "As I understand it early networks used this to squish the relevant weighted sum into that interval between zero and one", "start": 1039.18, "duration": 5.6}, {"text": "You know kind of motivated by this biological analogy of neurons either being inactive or active\n(Lisha) - Exactly", "start": 1044.98, "duration": 5.36}, {"text": "(3B1B) - But relatively few modern networks actually use sigmoid anymore. That's kind of old school right ?\n(Lisha) - Yeah or rather", "start": 1050.36, "duration": 5.96}, {"text": "ReLU seems to be much easier to train\n(3B1B) - And ReLU really stands for rectified linear unit", "start": 1056.37, "duration": 6.41}, {"text": "(Lisha) - Yes it's this kind of function where you're just taking a max of 0 and a where a is given by", "start": 1062.78, "duration": 6.059}, {"text": "what you were explaining in the video and what this was sort of motivated from I think was a", "start": 1069.12, "duration": 4.55}, {"text": "partially by a biological", "start": 1074.61, "duration": 2.0}, {"text": "Analogy with how", "start": 1076.62, "duration": 1.559}, {"text": "Neurons would either be activated or not and so if it passes a certain threshold", "start": 1078.179, "duration": 4.91}, {"text": "It would be the identity function", "start": 1083.25, "duration": 2.0}, {"text": "But if it did not then it would just not be activated so be zero so it's kind of a simplification", "start": 1085.29, "duration": 5.149}, {"text": "Using sigmoids didn't help training, or it was very difficult to train", "start": 1090.72, "duration": 3.709}, {"text": "It's at some point and people just tried relu and it happened to work", "start": 1094.429, "duration": 5.16}, {"text": "Very well for these incredibly", "start": 1100.11, "duration": 2.03}, {"text": "Deep neural networks.\n(3B1B) - All right", "start": 1102.69, "duration": 2.4}, {"text": "Thank You Lisha", "start": 1105.09, "duration": 0.97}, {"text": "for background amplify partners in early-stage VC invests in technical founders building the next generation of companies focused on the", "start": 1106.06, "duration": 7.369}, {"text": "applications of AI if you or someone that you know has ever thought about starting a company someday", "start": 1113.59, "duration": 4.819}, {"text": "Or if you're working on an early-stage one right now the Amplify folks would love to hear from you", "start": 1118.45, "duration": 4.729}, {"text": "they even set up a specific email for this video 3blue1brown@amplifypartners.com", "start": 1123.24, "duration": 5.56}, {"text": "so feel free to reach out to them through that", "start": 1128.8, "duration": 1.98}, {"text": " ", "start": 1152.13, "duration": 2.06}]