[{"text": "Last video I laid out the structure of a neural network", "start": 4.07, "duration": 2.989}, {"text": "I'll give a quick recap here just so that it's fresh in our minds", "start": 7.16, "duration": 2.929}, {"text": "And then I have two main goals for this video. The first is to introduce the idea of gradient descent,", "start": 10.089, "duration": 5.279}, {"text": "which underlies not only how neural networks learn,", "start": 15.65, "duration": 2.569}, {"text": "but how a lot of other machine learning works as well", "start": 18.22, "duration": 2.219}, {"text": "Then after that we're going to dig in a little more to how this particular network performs", "start": 20.66, "duration": 3.949}, {"text": "And what those hidden layers of neurons end up actually looking for", "start": 24.609, "duration": 3.149}, {"text": "As a reminder our goal here is the classic example of handwritten digit recognition", "start": 28.999, "duration": 4.49}, {"text": "the hello world of neural networks", "start": 34.129, "duration": 2.0}, {"text": "these digits are rendered on a 28 by 28 pixel grid each pixel with some grayscale value between 0 & 1", "start": 36.5, "duration": 6.59}, {"text": "those are what determine the activations of", "start": 43.61, "duration": 2.479}, {"text": "784 neurons in the input layer of the network and", "start": 46.85, "duration": 3.349}, {"text": "Then the activation for each neuron in the following layers is based on a weighted sum of", "start": 50.84, "duration": 4.879}, {"text": "All the activations in the previous layer plus some special number called a bias", "start": 56.0, "duration": 4.639}, {"text": "then you compose that sum with some other function like the sigmoid squishification or", "start": 61.699, "duration": 4.639}, {"text": "a ReLu the way that I walked through last video", "start": 66.4, "duration": 2.369}, {"text": "In total given the somewhat arbitrary choice of two hidden layers here with 16 neurons each the network has about", "start": 69.11, "duration": 6.619}, {"text": "13,000 weights and biases that we can adjust and it's these values that determine what exactly the network you know actually does", "start": 76.579, "duration": 7.58}, {"text": "Then what we mean when we say that this network classifies a given digit", "start": 84.799, "duration": 3.529}, {"text": "Is that the brightest of those 10 neurons in the final layer corresponds to that digit", "start": 88.329, "duration": 5.1}, {"text": "And remember the motivation that we had in mind here for the layered structure was that maybe", "start": 93.95, "duration": 4.639}, {"text": "The second layer could pick up on the edges and the third layer might pick up on patterns like loops and lines", "start": 98.78, "duration": 5.9}, {"text": "And the last one could just piece together those patterns to recognize digits", "start": 104.93, "duration": 3.799}, {"text": "So here we learn how the network learns", "start": 109.369, "duration": 2.66}, {"text": "What we want is an algorithm where you can show this network a whole bunch of training data", "start": 112.399, "duration": 4.7}, {"text": "which comes in the form of a bunch of different images of handwritten digits along with labels for what they're supposed to be and", "start": 117.229, "duration": 6.44}, {"text": "It'll adjust those", "start": 123.89, "duration": 1.769}, {"text": "13000 weights and biases so as to improve its performance on the training data", "start": 125.659, "duration": 4.13}, {"text": "Hopefully this layered structure will mean that what it learns", "start": 130.73, "duration": 2.839}, {"text": "generalizes to images beyond that training data", "start": 134.269, "duration": 2.45}, {"text": "And the way we test that is that after you train the network", "start": 136.72, "duration": 3.569}, {"text": "You show it more labeled theta that it's never seen before and you see how accurately it classifies those new images", "start": 140.29, "duration": 6.27}, {"text": "Fortunately for us and what makes this such a common example to start with is that the good people behind the MNIST base have", "start": 151.04, "duration": 5.96}, {"text": "put together a collection of tens of thousands of handwritten digit images each one labeled with the numbers that they're supposed to be and", "start": 157.0, "duration": 7.289}, {"text": "It's provocative as it is to describe a machine as learning once you actually see how it works", "start": 164.72, "duration": 4.819}, {"text": "It feels a lot less like some crazy sci-fi premise and a lot more like well a calculus exercise", "start": 169.54, "duration": 5.819}, {"text": "I mean basically it comes down to finding the minimum of a certain function", "start": 175.39, "duration": 4.199}, {"text": "Remember conceptually we're thinking of each neuron as being connected", "start": 181.519, "duration": 3.68}, {"text": "to all of the neurons in the previous layer and the weights in the weighted sum defining its activation are kind of like the", "start": 185.39, "duration": 6.919}, {"text": "strengths of those connections", "start": 192.44, "duration": 1.62}, {"text": "And the bias is some indication of whether that neuron tends to be active or inactive and to start things off", "start": 194.06, "duration": 6.38}, {"text": "We're just gonna initialize all of those weights and biases totally randomly needless to say this network is going to perform", "start": 200.44, "duration": 6.479}, {"text": "pretty horribly on a given training example since it's just doing something random for example you feed in this image of a 3 and the", "start": 206.919, "duration": 6.84}, {"text": "Output layer it just looks like a mess", "start": 213.76, "duration": 2.039}, {"text": "So what you do is you define a cost function a way of telling the computer: \"No bad computer!", "start": 216.349, "duration": 6.169}, {"text": "That output should have activations which are zero for most neurons, but one for this neuron what you gave me is utter trash\"", "start": 222.739, "duration": 7.79}, {"text": "To say that a little more mathematically what you do is add up the squares of the differences between", "start": 231.26, "duration": 5.27}, {"text": "each of those trash output activations and the value that you want them to have and", "start": 236.72, "duration": 4.699}, {"text": "This is what we'll call the cost of a single training example", "start": 241.489, "duration": 3.11}, {"text": "Notice this sum is small when the network confidently classifies the image correctly", "start": 245.599, "duration": 5.15}, {"text": "But it's large when the network seems like it doesn't really know what it's doing", "start": 252.199, "duration": 3.44}, {"text": "So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal", "start": 258.33, "duration": 6.919}, {"text": "This average cost is our measure for how lousy the network is and how bad the computer should feel, and that's a complicated thing", "start": 267.06, "duration": 7.25}, {"text": "Remember how the network itself was basically a function one that takes in", "start": 274.83, "duration": 4.13}, {"text": "784 numbers as inputs the pixel values and spits out ten numbers as its output and in a sense", "start": 279.54, "duration": 6.35}, {"text": "It's parameterised by all these weights and biases", "start": 285.89, "duration": 2.88}, {"text": "While the cost function is a layer of complexity on top of that it takes as its input", "start": 289.14, "duration": 4.88}, {"text": "those thirteen thousand or so weights and biases and it spits out a single number describing how bad those weights and biases are and", "start": 294.45, "duration": 7.609}, {"text": "The way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data", "start": 302.34, "duration": 6.409}, {"text": "That's a lot to think about", "start": 309.15, "duration": 2.0}, {"text": "But just telling the computer what a crappy job, it's doing isn't very helpful", "start": 312.0, "duration": 3.619}, {"text": "You want to tell it how to change those weights and biases so that it gets better?", "start": 315.9, "duration": 3.919}, {"text": "To make it easier rather than struggling to imagine a function with 13,000 inputs", "start": 320.82, "duration": 4.309}, {"text": "Just imagine a simple function that has one number as an input and one number as an output", "start": 325.13, "duration": 5.279}, {"text": "How do you find an input that minimizes the value of this function?", "start": 330.96, "duration": 4.039}, {"text": "Calculus students will know that you can sometimes figure out that minimum explicitly", "start": 336.27, "duration": 3.769}, {"text": "But that's not always feasible for really complicated functions", "start": 340.26, "duration": 3.619}, {"text": "Certainly not in the thirteen thousand input version of this situation for our crazy complicated neural network cost function", "start": 344.31, "duration": 7.85}, {"text": "A more flexible tactic is to start at any old input and figure out which direction you should step to make that output lower", "start": 352.35, "duration": 6.679}, {"text": "Specifically if you can figure out the slope of the function where you are", "start": 360.12, "duration": 3.59}, {"text": "Then shift to the left if that slope is positive and shift the input to the right if that slope is negative", "start": 364.02, "duration": 5.599}, {"text": "If you do this repeatedly at each point checking the new slope and taking the appropriate step", "start": 372.13, "duration": 4.669}, {"text": "you're gonna approach some local minimum of the function and", "start": 376.8, "duration": 3.239}, {"text": "the image you might have in mind here is a ball rolling down a hill and", "start": 380.28, "duration": 3.8}, {"text": "Notice even for this really simplified single input function there are many possible valleys that you might land in", "start": 384.4, "duration": 6.5}, {"text": "Depending on which random input you start at and there's no guarantee that the local minimum", "start": 391.54, "duration": 4.68}, {"text": "You land in is going to be the smallest possible value of the cost function", "start": 396.58, "duration": 2.46}, {"text": "That's going to carry over to our neural network case as well, and I also want you to notice", "start": 399.61, "duration": 4.399}, {"text": "How if you make your step sizes proportional to the slope", "start": 404.01, "duration": 3.18}, {"text": "Then when the slope is flattening out towards the minimum your steps get smaller and smaller and that kind of helps you from overshooting", "start": 407.62, "duration": 6.92}, {"text": "Bumping up the complexity a bit imagine instead a function with two inputs and one output", "start": 415.72, "duration": 4.729}, {"text": "You might think of the input space as the XY plane and the cost function as being graphed as a surface above it", "start": 421.12, "duration": 6.619}, {"text": "Now instead of asking about the slope of the function you have to ask which direction should you step in this input space?", "start": 428.23, "duration": 6.83}, {"text": "So as to decrease the output of the function most quickly in other words. What's the downhill direction?", "start": 435.31, "duration": 7.13}, {"text": "And again it's helpful to think of a ball rolling down that hill", "start": 442.44, "duration": 2.939}, {"text": "Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent", "start": 446.26, "duration": 7.82}, {"text": "Basically, which direction should you step to increase the function most quickly", "start": 454.75, "duration": 3.709}, {"text": "naturally enough taking the negative of that gradient gives you the direction to step that decreases the function most quickly and", "start": 459.1, "duration": 7.339}, {"text": "Even more than that the length of this gradient vector is actually an indication for just how steep that steepest slope is", "start": 467.02, "duration": 6.38}, {"text": "Now if you're unfamiliar with multivariable calculus", "start": 474.13, "duration": 2.15}, {"text": "And you want to learn more check out some of the work that I did for Khan Academy on the topic", "start": 476.28, "duration": 3.959}, {"text": "Honestly, though all that matters for you and me right now", "start": 480.91, "duration": 2.869}, {"text": "Is that in principle there exists a way to compute this vector. This vector that tells you what the", "start": 483.78, "duration": 5.639}, {"text": "Downhill direction is and how steep it is you'll be okay if that's all you know and you're not rock solid on the details", "start": 489.52, "duration": 6.38}, {"text": "because if you can get that the algorithm from minimizing the function is to compute this gradient direction then take a small step downhill and", "start": 496.79, "duration": 7.79}, {"text": "Just repeat that over and over", "start": 504.74, "duration": 2.0}, {"text": "It's the same basic idea for a function that has 13,000 inputs instead of two inputs imagine organizing all", "start": 507.8, "duration": 6.8}, {"text": "13,000 weights and biases of our network into a giant column vector", "start": 515.33, "duration": 4.07}, {"text": "The negative gradient of the cost function is just a vector", "start": 519.68, "duration": 4.19}, {"text": "It's some Direction inside this insanely huge input space that tells you which", "start": 523.88, "duration": 5.419}, {"text": "nudges to all of those numbers is going to cause the most rapid decrease to the cost function and", "start": 529.4, "duration": 5.63}, {"text": "of course with our specially designed cost function", "start": 535.46, "duration": 2.69}, {"text": "Changing the weights and biases to decrease it means making the output of the network on each piece of training data", "start": 538.58, "duration": 6.32}, {"text": "Look less like a random array of ten values and more like an actual decision that we want it to make", "start": 545.18, "duration": 5.419}, {"text": "It's important to remember this cost function involves an average over all of the training data", "start": 551.03, "duration": 5.0}, {"text": "So if you minimize it it means it's a better performance on all of those samples", "start": 556.37, "duration": 4.22}, {"text": "The algorithm for computing this gradient efficiently which is effectively the heart of how a neural network learns is called back propagation", "start": 563.78, "duration": 7.069}, {"text": "And it's what I'm going to be talking about next video", "start": 571.19, "duration": 3.5}, {"text": "There I really want to take the time to walk through", "start": 574.69, "duration": 2.0}, {"text": "What exactly happens to each weight and each bias for a given piece of training data?", "start": 576.83, "duration": 4.609}, {"text": "Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas", "start": 581.81, "duration": 5.15}, {"text": "Right here right now the main thing. I want you to know independent of implementation details", "start": 587.51, "duration": 4.669}, {"text": "is that what we mean when we talk about a network learning is that it's just minimizing a cost function and", "start": 592.18, "duration": 6.299}, {"text": "Notice one consequence of that is that it's important for this cost function to have a nice smooth output", "start": 598.94, "duration": 5.539}, {"text": "So that we can find a local minimum by taking little steps downhill", "start": 604.48, "duration": 3.33}, {"text": "This is why by the way", "start": 608.81, "duration": 1.71}, {"text": "Artificial neurons have continuously ranging activations rather than simply being active or inactive in a binary way", "start": 610.52, "duration": 6.229}, {"text": "if the way that biological neurons are", "start": 616.75, "duration": 2.0}, {"text": "This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent", "start": 619.94, "duration": 6.83}, {"text": "It's a way to converge towards some local minimum of a cost function basically a valley in this graph", "start": 626.93, "duration": 5.45}, {"text": "I'm still showing the picture of a function with two inputs of course because nudges in a thirteen thousand dimensional input", "start": 632.93, "duration": 5.96}, {"text": "Space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this", "start": 638.89, "duration": 5.159}, {"text": "Each component of the negative gradient tells us two things the sign of course tells us whether the corresponding", "start": 644.63, "duration": 6.71}, {"text": "Component of the input vector should be nudged up or down, but importantly the relative magnitudes of all these components", "start": 651.83, "duration": 7.309}, {"text": "Kind of tells you which changes matter more", "start": 659.84, "duration": 2.69}, {"text": "You see in our network an adjustment to one of the weights might have a much greater", "start": 665.15, "duration": 4.19}, {"text": "impact on the cost function than the adjustment to some other weight", "start": 669.71, "duration": 3.229}, {"text": "Some of these connections just matter more for our training data", "start": 674.45, "duration": 3.5}, {"text": "So a way that you can think about this gradient vector of our mind-warpingly", "start": 678.92, "duration": 3.77}, {"text": "massive cost function is that it encodes the relative importance of each weight and bias", "start": 682.69, "duration": 5.309}, {"text": "That is which of these changes is going to carry the most bang for your buck", "start": 688.25, "duration": 3.95}, {"text": "This really is just another way of thinking about direction", "start": 693.56, "duration": 2.9}, {"text": "To take a simpler example if you have some function with two variables as an input and you", "start": 696.86, "duration": 4.43}, {"text": "Compute that its gradient at some particular point comes out as (3,1)", "start": 701.69, "duration": 4.85}, {"text": "Then on the one hand you can interpret that as saying that when you're standing at that input", "start": 707.42, "duration": 4.25}, {"text": "moving along this direction increases the function most quickly", "start": 712.07, "duration": 3.08}, {"text": "That when you graph the function above the plane of input points that vector is what's giving you the straight uphill direction", "start": 715.46, "duration": 6.769}, {"text": "But another way to read that is to say that changes to this first variable", "start": 722.6, "duration": 3.98}, {"text": "Have three times the importance as changes to the second variable that at least in the neighborhood of the relevant input", "start": 726.74, "duration": 6.65}, {"text": "Nudging the x value carries a lot more bang for your buck", "start": 733.52, "duration": 3.169}, {"text": "All right", "start": 739.31, "duration": 0.62}, {"text": "Let's zoom out and sum up where we are so far the network itself is this function with", "start": 739.93, "duration": 5.01}, {"text": "784 inputs and 10 outputs defined in terms of all of these weighted sums", "start": 745.4, "duration": 4.459}, {"text": "the cost function is a layer of complexity on top of that it takes the", "start": 750.35, "duration": 4.43}, {"text": "13,000 weights and biases as inputs and spits out a single measure of lousyness based on the training examples and", "start": 755.12, "duration": 6.75}, {"text": "The gradient of the cost function is one more layer of complexity still it tells us", "start": 762.18, "duration": 5.75}, {"text": "What nudges to all of these weights and biases cause the fastest change to the value of the cost function", "start": 767.93, "duration": 5.909}, {"text": "Which you might interpret is saying which changes to which weights matter the most", "start": 773.97, "duration": 3.71}, {"text": "So when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process", "start": 782.55, "duration": 6.739}, {"text": "How well does it actually perform on images that it's never seen before?", "start": 789.42, "duration": 3.529}, {"text": "Well the one that I've described here with the two hidden layers of sixteen neurons each chosen mostly for aesthetic reasons", "start": 793.68, "duration": 5.929}, {"text": "well, it's not bad it classifies about 96 percent of the new images that it sees correctly and", "start": 800.579, "duration": 5.51}, {"text": "Honestly, if you look at some of the examples that it messes up on you kind of feel compelled to cut it a little slack", "start": 806.759, "duration": 5.48}, {"text": "Now if you play around with the hidden layer structure and make a couple tweaks", "start": 815.759, "duration": 3.32}, {"text": "You can get this up to 98% and that's pretty good. It's not the best", "start": 819.079, "duration": 4.619}, {"text": "You can certainly get better performance by getting more sophisticated than this plain vanilla Network", "start": 823.74, "duration": 4.669}, {"text": "But given how daunting the initial task is I just think there's something?", "start": 828.569, "duration": 4.1}, {"text": "Incredible about any network doing this well on images that it's never seen before", "start": 832.889, "duration": 4.04}, {"text": "Given that we never specifically told it what patterns to look for", "start": 837.389, "duration": 3.53}, {"text": "Originally the way that I motivated this structure was by describing a hope that we might have", "start": 842.579, "duration": 4.489}, {"text": "That the second layer might pick up on little edges", "start": 847.259, "duration": 2.48}, {"text": "That the third layer would piece together those edges to recognize loops and longer lines and that those might be pieced together to recognize digits", "start": 849.809, "duration": 7.28}, {"text": "So is this what our network is actually doing? Well for this one at least", "start": 857.699, "duration": 5.03}, {"text": "Not at all", "start": 863.339, "duration": 1.11}, {"text": "remember how last video we looked at how the weights of the", "start": 864.449, "duration": 2.96}, {"text": "Connections from all of the neurons in the first layer to a given neuron in the second layer", "start": 867.48, "duration": 4.369}, {"text": "Can be visualized as a given pixel pattern that that second layer neuron is picking up on", "start": 871.98, "duration": 4.849}, {"text": "Well when we actually do that for the weights associated with these transitions from the first layer to the next", "start": 877.35, "duration": 5.959}, {"text": "Instead of picking up on isolated little edges here and there. They look well almost random", "start": 883.709, "duration": 6.5}, {"text": "Just put some very loose patterns in the middle there it would seem that in the unfathomably large", "start": 890.37, "duration": 6.029}, {"text": "13,000 dimensional space of possible weights and biases our network found itself a happy little local minimum that", "start": 896.92, "duration": 5.66}, {"text": "despite successfully classifying most images doesn't exactly pick up on the patterns that we might have hoped for and", "start": 902.86, "duration": 6.08}, {"text": "To really drive this point home watch what happens when you input a random image", "start": 909.43, "duration": 4.279}, {"text": "if the system was smart you might expect it to either feel uncertain maybe not really activating any of those 10 output neurons or", "start": 914.019, "duration": 7.43}, {"text": "Activating them all evenly", "start": 921.579, "duration": 1.621}, {"text": "But instead it", "start": 923.2, "duration": 1.62}, {"text": "Confidently gives you some nonsense answer as if it feels as sure that this random noise is a 5 as it does that an actual", "start": 924.82, "duration": 7.19}, {"text": "image of a 5 is a 5", "start": 932.01, "duration": 2.0}, {"text": "phrase differently even if this network can recognize digits pretty well it has no idea how to draw them a", "start": 934.18, "duration": 6.649}, {"text": "Lot of this is because it's such a tightly constrained training setup", "start": 941.5, "duration": 3.649}, {"text": "I mean put yourself in the network's shoes here from its point of view the entire universe consists of nothing", "start": 945.149, "duration": 6.33}, {"text": "But clearly defined unmoving digits centered in a tiny grid and its cost function just never gave it any", "start": 951.48, "duration": 6.059}, {"text": "Incentive to be anything, but utterly confident in its decisions", "start": 957.7, "duration": 3.259}, {"text": "So if this is the image of what those second layer neurons are really doing", "start": 961.69, "duration": 3.38}, {"text": "You might wonder why I would introduce this network with the motivation of picking up on edges and patterns", "start": 965.14, "duration": 4.699}, {"text": "I mean, that's just not at all what it ends up doing", "start": 969.839, "duration": 2.13}, {"text": "Well, this is not meant to be our end goal, but instead a starting point frankly", "start": 973.029, "duration": 4.88}, {"text": "This is old technology", "start": 977.91, "duration": 1.21}, {"text": "the kind researched in the 80s and 90s and", "start": 979.12, "duration": 2.39}, {"text": "You do need to understand it before you can understand more detailed modern variants and it clearly is capable of solving some interesting problems", "start": 981.64, "duration": 7.489}, {"text": "But the more you dig in to what those hidden layers are really doing the less intelligent it seems", "start": 989.41, "duration": 4.7}, {"text": "Shifting the focus for a moment from how networks learn to how you learn", "start": 998.53, "duration": 3.829}, {"text": "That'll only happen if you engage actively with the material here somehow", "start": 1002.58, "duration": 3.559}, {"text": "One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what", "start": 1006.66, "duration": 6.44}, {"text": "Changes you might make to this system", "start": 1013.44, "duration": 1.79}, {"text": "And how it perceives images if you wanted it to better pick up on things like edges and patterns?", "start": 1015.23, "duration": 5.489}, {"text": "But better than that to actually engage with the material", "start": 1021.36, "duration": 3.05}, {"text": "I", "start": 1024.41, "duration": 0.669}, {"text": "Highly recommend the book by Michael Nielsen on deep learning and neural networks", "start": 1025.079, "duration": 3.89}, {"text": "In it you can find the code and the data to download and play with for this exact example", "start": 1029.19, "duration": 5.179}, {"text": "And the book will walk you through step by step what that code is doing", "start": 1034.41, "duration": 3.679}, {"text": "What's awesome is that this book is free and publicly available", "start": 1038.91, "duration": 2.839}, {"text": "So if you do get something out of it consider joining me in making a donation towards Nielsen's efforts", "start": 1042.36, "duration": 5.18}, {"text": "I've also linked a couple other resources that I like a lot in the description including the", "start": 1047.91, "duration": 4.309}, {"text": "phenomenal and beautiful blog post by Chris Ola and the articles in distill", "start": 1052.47, "duration": 3.92}, {"text": "To close things off here for the last few minutes", "start": 1058.23, "duration": 1.97}, {"text": "I want to jump back into a snippet of the interview that I had with Leisha Lee", "start": 1060.2, "duration": 3.54}, {"text": "You might remember her from the last video. She did her PhD work in deep learning and in this little snippet", "start": 1063.93, "duration": 5.149}, {"text": "She talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning", "start": 1069.08, "duration": 6.45}, {"text": "Just to set up where we were in the conversation the first paper took one of these particularly deep neural networks", "start": 1075.81, "duration": 5.539}, {"text": "That's really good at image recognition and instead of training it on a properly labeled data", "start": 1081.35, "duration": 4.56}, {"text": "Set it shuffled all of the labels around before training", "start": 1085.91, "duration": 2.669}, {"text": "Obviously the testing accuracy here was going to be no better than random since everything's just randomly labeled", "start": 1088.8, "duration": 5.869}, {"text": "But it was still able to achieve the same training accuracy as you would on a properly labeled dataset", "start": 1094.8, "duration": 6.079}, {"text": "Basically the millions of weights for this particular network were enough for it to just memorize the random data", "start": 1101.49, "duration": 6.05}, {"text": "Which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image?", "start": 1107.82, "duration": 6.559}, {"text": "Or is it just you know?", "start": 1114.38, "duration": 2.0}, {"text": "memorize the entire", "start": 1116.52, "duration": 0.9}, {"text": "Data set of what the correct classification is and so a couple of you know half a year later at ICML this year", "start": 1117.42, "duration": 6.439}, {"text": "There was not exactly rebuttal paper paper that addressed some asked like hey", "start": 1124.47, "duration": 4.569}, {"text": "Actually these networks are doing something a little bit smarter than that if you look at that accuracy curve", "start": 1129.47, "duration": 5.809}, {"text": "if you were just training on a", "start": 1135.279, "duration": 2.22}, {"text": "Random data set that curve sort of went down very you know very slowly in almost kind of a linear fashion", "start": 1138.259, "duration": 6.92}, {"text": "So you're really struggling to find that local minima of possible", "start": 1145.179, "duration": 4.41}, {"text": "you know the right weights that would get you that accuracy whereas if you're actually training on a structured data set one that has the", "start": 1149.59, "duration": 5.699}, {"text": "Right labels. You know you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that", "start": 1155.289, "duration": 6.15}, {"text": "Accuracy level and so in some sense it was easier to find that", "start": 1162.2, "duration": 3.949}, {"text": "Local maxima and so it was also interesting about that is it caught brings into light another paper from actually a couple of years ago", "start": 1166.759, "duration": 7.19}, {"text": "Which has a lot more", "start": 1174.08, "duration": 2.0}, {"text": "simplifications about the network layers", "start": 1176.99, "duration": 2.179}, {"text": "But one of the results was saying how if you look at the optimization landscape the local minima that these networks tend to learn are", "start": 1179.169, "duration": 7.619}, {"text": "Actually of equal quality so in some sense if your data set is structure, and you should be able to find that much more easily", "start": 1187.34, "duration": 6.739}, {"text": "My thanks as always to those of you supporting on patreon", "start": 1198.139, "duration": 3.05}, {"text": "I've said before just what a game-changer patreon is but these videos really would not be possible without you I", "start": 1201.19, "duration": 5.76}, {"text": "Also want to give a special. Thanks to the VC firm amplifi partners in their support of these initial videos in the series", "start": 1207.23, "duration": 5.659}, {"text": "They focus on very early stage machine learning and AI companies", "start": 1213.47, "duration": 3.679}, {"text": "and I feel pretty confident in the", "start": 1217.309, "duration": 1.801}, {"text": "Probabilities that some of you watching this and even more likely some of the people that you know are", "start": 1219.11, "duration": 4.789}, {"text": "right now in the early stages of getting such a company off the ground and", "start": 1224.21, "duration": 3.739}, {"text": "The amplifi folks would love to hear from any such founders", "start": 1228.23, "duration": 3.049}, {"text": "and they even set up an email address just for this video that you can reach out to them through three blue one brown at", "start": 1231.279, "duration": 5.79}, {"text": "amplify partners com", "start": 1237.59, "duration": 2.0}]