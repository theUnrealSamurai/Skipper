[{"text": "Thanks to Curiosity Stream for supporting\nPBS Digital Studios.", "start": 0.18, "duration": 3.58}, {"text": "Hey, I\u2019m Jabril and welcome to Crash Course AI!", "start": 3.76, "duration": 4.26}, {"text": "Language is one of the most impressive things\nhumans do.", "start": 8.02, "duration": 2.8}, {"text": "It\u2019s how I\u2019m transferring knowledge from\nmy brain to yours right this second!", "start": 10.82, "duration": 4.38}, {"text": "Languages come in many shapes and sizes, they\ncan be spoken or written, and are made up", "start": 15.2, "duration": 4.47}, {"text": "of different components like sentences, words,\nand characters that vary across cultures.", "start": 19.67, "duration": 6.82}, {"text": "For instance, English has 26 letters and Chinese\nhas tens-of-thousands of characters.", "start": 26.49, "duration": 6.74}, {"text": "So far, a lot of the problems we\u2019ve been\nsolving with AI and machine learning technologies", "start": 33.23, "duration": 3.83}, {"text": "have involved processing images, but the most\ncommon way that most of us interact with computers", "start": 37.06, "duration": 5.97}, {"text": "is through language.", "start": 43.03, "duration": 1.23}, {"text": "We type questions into search engines, we\ntalk to our smartphones to set alarms, and", "start": 44.26, "duration": 4.85}, {"text": "sometimes we even get a little help with our\nSpanish homework from Google Translate.", "start": 49.11, "duration": 4.449}, {"text": "So today, we\u2019re going to explore the field\nof Natural Language Processing.", "start": 53.56, "duration": 4.92}, {"text": "INTRO", "start": 58.48, "duration": 8.9}, {"text": "Natural Language Processing, or NLP, mainly\nexplores two big ideas.", "start": 67.38, "duration": 5.74}, {"text": "First, there\u2019s Natural Language Understanding,\nor how we get meaning out of combinations", "start": 73.12, "duration": 4.92}, {"text": "of letters.", "start": 78.04, "duration": 1.23}, {"text": "These are AI that filter your spam emails,\nfigure out if that Amazon search for \u201capple\u201d", "start": 79.27, "duration": 4.25}, {"text": "was grocery or computer shopping, or instruct\nyour self-driving car how to get to a friend\u2019s", "start": 83.52, "duration": 4.88}, {"text": "house.", "start": 88.4, "duration": 1.0}, {"text": "And second, there\u2019s Natural Language Generation,\nor how to generate language from knowledge.", "start": 89.4, "duration": 5.41}, {"text": "These are AI that perform translations, summarize\ndocuments, or chat with you.", "start": 94.81, "duration": 4.629}, {"text": "The key to both problems is understanding\nthe meaning of a word, which is tricky because", "start": 99.439, "duration": 5.29}, {"text": "words have no meaning on their own.", "start": 104.729, "duration": 2.221}, {"text": "We assign meaning to symbols.", "start": 106.95, "duration": 2.349}, {"text": "To make things even harder, in many cases,\nlanguage can be ambiguous and the meaning", "start": 109.299, "duration": 5.421}, {"text": "of a word depends on the context it\u2019s used\nin", "start": 114.72, "duration": 3.079}, {"text": "If I tell you to meet me at the bank, without\nany context, I could mean the river bank or", "start": 117.799, "duration": 5.14}, {"text": "the place where I\u2019m grabbing some cash.", "start": 122.939, "duration": 1.991}, {"text": "If I say \u201cThis fridge is great!\u201d, that\u2019s\na totally different meaning from \u201cThis fridge", "start": 124.93, "duration": 5.29}, {"text": "was *great*, it lasted a whole week before\nbreaking.\u201d", "start": 130.22, "duration": 3.859}, {"text": "So, how did we learn to attach meaning to\nsounds?", "start": 134.079, "duration": 3.351}, {"text": "How do we know great [enthusiastic] means\nsomething different from great [sarcastic]?", "start": 137.43, "duration": 4.669}, {"text": "Well, even though there\u2019s nothing inherent\nin the word \u201ccat\u201d that tells us it\u2019s", "start": 142.099, "duration": 3.9}, {"text": "soft, purrs, and chases mice\u2026 when we were\nkids, someone probably told us \u201cthis is", "start": 145.999, "duration": 5.151}, {"text": "a cat.\u201d", "start": 151.15, "duration": 1.169}, {"text": "Or a gato, m\u0101o, billee, qut.", "start": 152.32, "duration": 3.46}, {"text": "When we\u2019re solving a natural language processing\nproblem, whether it\u2019s natural language understanding", "start": 155.78, "duration": 4.459}, {"text": "or natural language generation, we have to\nthink about how our AI is going to learn the", "start": 160.239, "duration": 4.73}, {"text": "meaning of words and understand our potential\nmistakes.", "start": 164.969, "duration": 3.691}, {"text": "Sometimes we can compare words by looking\nat the letters they share.", "start": 168.66, "duration": 4.139}, {"text": "This works well if a word has morphology.", "start": 172.799, "duration": 2.92}, {"text": "Take the root word \u201cswim\u201d for example.We\ncan modify it with rules so if someone\u2019s", "start": 175.719, "duration": 5.24}, {"text": "doing it right now, they\u2019re swimming, or\nthe person doing the action is the swimmer.", "start": 180.959, "duration": 6.161}, {"text": "Drinking, drinker, thinking, thinker, \u2026 you\nget the idea.", "start": 187.12, "duration": 4.0}, {"text": "But we can\u2019t use morphology for all words,\nlike how knowing that a van is a vehicle doesn\u2019t", "start": 191.12, "duration": 5.949}, {"text": "let us know that a vandal smashed in a car\nwindow.", "start": 197.069, "duration": 2.831}, {"text": "Many words that are really similar, like cat\nand car, are completely unrelated.", "start": 199.9, "duration": 4.899}, {"text": "And on the other hand, cat and Felidae (the word for the scientific family of cats) mean very", "start": 204.8, "duration": 5.409}, {"text": "similar things and only share one letter!", "start": 210.209, "duration": 2.81}, {"text": "One common way to guess that words have similar\nmeaning is using distributional semantics,", "start": 213.019, "duration": 5.22}, {"text": "or seeing which words appear in the same sentences\na lot.", "start": 218.239, "duration": 3.191}, {"text": "This is one of many cases where NLP relies\non insights from the field of linguistics.", "start": 221.43, "duration": 5.869}, {"text": "As the linguist John Firth once said, \u201cYou\nshall know a word by the company it keeps.\u201d", "start": 227.299, "duration": 5.17}, {"text": "But to make computers understand distributional\nsemantics, we have to express the concept", "start": 232.469, "duration": 4.8}, {"text": "in math.", "start": 237.269, "duration": 1.0}, {"text": "One simple technique is to use count vectors.", "start": 238.269, "duration": 3.021}, {"text": "A count vector is the number of times a word\nappears in the same article or sentence as", "start": 241.29, "duration": 5.099}, {"text": "other common words.", "start": 246.389, "duration": 1.721}, {"text": "If two words show up in the same sentence,\nthey probably have pretty similar meanings.", "start": 248.11, "duration": 4.699}, {"text": "So let\u2019s say we asked an algorithm to compare\nthree words, car, cat, and Felidae, using", "start": 252.809, "duration": 6.18}, {"text": "count vectors to guess which ones have similar\nmeaning.", "start": 258.989, "duration": 2.87}, {"text": "We could download the beginning of the Wikipedia\npages for each word to see which /other/ words", "start": 261.859, "duration": 4.381}, {"text": "show up.", "start": 266.24, "duration": 1.0}, {"text": "Here\u2019s what we got:", "start": 267.24, "duration": 1.06}, {"text": "And a lot of the top words are all the same:\nthe, and, of, in.", "start": 268.3, "duration": 6.619}, {"text": "These are all function words or stop words,\nwhich help define the structure of language,", "start": 274.919, "duration": 5.5}, {"text": "and help convey precise meaning.", "start": 280.419, "duration": 2.47}, {"text": "Like how \u201can apple\u201d means any apple, but\n\u201cthe apple\u201d specifies one in particular.", "start": 282.889, "duration": 5.451}, {"text": "But, because they change the meaning of another\nword, they don\u2019t have much meaning by themselves,", "start": 288.34, "duration": 5.299}, {"text": "so we\u2019ll remove them for now, and simplify\nplurals and conjugations.", "start": 293.639, "duration": 4.78}, {"text": "Let\u2019s try it again:", "start": 298.419, "duration": 1.75}, {"text": "Based on this, it looks like cat and Felidae\nmean almost the same thing, because they both", "start": 300.169, "duration": 4.821}, {"text": "show up with lots of the same words in their\nWikipedia articles!", "start": 304.99, "duration": 3.66}, {"text": "And neither of them mean the same thing as\ncar.", "start": 308.65, "duration": 3.109}, {"text": "But this is also a really simplified example.", "start": 311.759, "duration": 3.451}, {"text": "One of the problems with count vectors is\nthat we have to store a LOT of data.", "start": 315.21, "duration": 4.139}, {"text": "To compare a bunch of words using counts like\nthis, we\u2019d need a massive list of every", "start": 319.349, "duration": 4.53}, {"text": "word we\u2019ve ever seen in the same sentence,\nand that\u2019s unmanageable.", "start": 323.88, "duration": 4.7}, {"text": "So, we\u2019d like to learn a representation\nfor words that captures all the same relationships", "start": 328.58, "duration": 4.639}, {"text": "and similarities as count vectors but is much\nmore compact.", "start": 333.219, "duration": 4.57}, {"text": "In the unsupervised learning episode, we talked\nabout how to compare images by building representations", "start": 337.789, "duration": 4.821}, {"text": "of those images.", "start": 342.61, "duration": 1.68}, {"text": "We needed a model that could build internal\nrepresentations and that could generate predictions.", "start": 344.29, "duration": 5.129}, {"text": "And we can do the same thing for words.", "start": 349.419, "duration": 2.691}, {"text": "This is called an encoder-decoder model: the\nencoder tells us what we should think and", "start": 352.11, "duration": 5.33}, {"text": "remember about what we just read...", "start": 357.44, "duration": 2.0}, {"text": "and the decoder uses that thought to decide\nwhat we want to say or do.", "start": 359.44, "duration": 4.909}, {"text": "We\u2019re going to start with a simple version\nof this framework.", "start": 364.349, "duration": 3.021}, {"text": "Let\u2019s create a little game of fill in the\nblank to see what basic pieces we need to", "start": 367.37, "duration": 4.079}, {"text": "train an unsupervised learning model.", "start": 371.449, "duration": 2.661}, {"text": "This is a simple task called language modeling.", "start": 374.11, "duration": 3.059}, {"text": "If I have the sentence:", "start": 377.169, "duration": 1.12}, {"text": "I\u2019m kinda hungry, I think I\u2019d like some\nchocolate _____ .", "start": 378.289, "duration": 4.12}, {"text": "What are the most likely words that can go\nin that spot?", "start": 382.409, "duration": 3.32}, {"text": "And how might we train a model to encode the\nsentence and decode a guess for the blank?", "start": 385.729, "duration": 4.581}, {"text": "In this example, I can guess the answer might\nbe \u201ccake\u201d or \u201cmilk\u201d but probably not", "start": 390.31, "duration": 5.099}, {"text": "something like \u201cpotatoes,\u201d because I\u2019ve\nnever heard of \u201cchocolate potatoes\u201d so", "start": 395.409, "duration": 4.85}, {"text": "they probably don\u2019t exist.", "start": 400.259, "duration": 2.581}, {"text": "Definitely don\u2019t exist.", "start": 402.84, "duration": 2.609}, {"text": "That should not be a thing.", "start": 405.449, "duration": 1.171}, {"text": "The group of words that can fill in that blank is an unsupervised cluster that an AI could use.", "start": 406.62, "duration": 5.36}, {"text": "So for this sentence, our encoder might only\nneed to focus on the word chocolate so the", "start": 411.98, "duration": 4.96}, {"text": "decoder has a cluster of \u201cchocolate food\nwords\u201d to pull from to fill in the blank.", "start": 416.94, "duration": 5.59}, {"text": "Now let\u2019s try a harder example:", "start": 422.53, "duration": 2.37}, {"text": "Dianna, a friend of mine from San Diego who\nreally loves physics, is having a birthday", "start": 424.9, "duration": 4.519}, {"text": "party next week, so I want to find a present\nfor ____.", "start": 429.419, "duration": 4.381}, {"text": "When I read this sentence, my brain identifies\nand remembers two things: First, that we\u2019re", "start": 433.8, "duration": 5.06}, {"text": "talking about Dianna from 27 words ago!", "start": 438.86, "duration": 2.709}, {"text": "And second, that my friend Dianna uses the\npronoun \u201cher.\u201d", "start": 441.569, "duration": 3.761}, {"text": "That means we want our encoder to build a\nrepresentation that captures all these pieces", "start": 445.33, "duration": 4.28}, {"text": "of information from the sentence, so the decoder\ncan choose the right word for the blank.", "start": 449.61, "duration": 5.55}, {"text": "And if we keep the sentence going:", "start": 455.16, "duration": 1.98}, {"text": "Dianna, a friend of mine from San Diego who\nreally loves physics, is having a birthday", "start": 457.15, "duration": 4.41}, {"text": "party next week, so I want to find a present\nfor her that has to do with _____ .", "start": 461.56, "duration": 5.129}, {"text": "Now, I can remember that Dianna likes physics\nfrom earlier in the sentence.", "start": 466.689, "duration": 4.13}, {"text": "So we\u2019d like our encoder to remember that\ntoo, so that the decoder can use that information", "start": 470.819, "duration": 4.791}, {"text": "to guess the answer.", "start": 475.61, "duration": 1.359}, {"text": "So we can see how the representation the model\nbuilds really has to remember key details", "start": 476.969, "duration": 4.35}, {"text": "of what we\u2019ve said or heard.", "start": 481.32, "duration": 2.24}, {"text": "And there\u2019s a limit to how much a model\ncan remember.", "start": 483.56, "duration": 2.8}, {"text": "Professor Ray Mooney has famously said that\nwe\u2019ll \u201cnever fit the whole meaning of", "start": 486.36, "duration": 3.88}, {"text": "a sentence into a single vector\u201d and we\nstill don\u2019t know if we can.", "start": 490.24, "duration": 4.929}, {"text": "Professor Mooney may be right, but that doesn\u2019t\nmean we can\u2019t make something useful.", "start": 495.169, "duration": 4.671}, {"text": "So so far we\u2019ve been using words.", "start": 499.84, "duration": 3.09}, {"text": "But computers don\u2019t work words quite\nlike this.", "start": 502.93, "duration": 3.289}, {"text": "So let\u2019s step away from our high level view\nof language modeling and try to predict the", "start": 506.219, "duration": 3.81}, {"text": "next word in a sentence anyway with a neural\nnetwork.", "start": 510.029, "duration": 3.491}, {"text": "To do this, our data will be lots of sentences\nwe collect from things like someone speaking", "start": 513.52, "duration": 5.26}, {"text": "or text from books.", "start": 518.789, "duration": 1.771}, {"text": "Then, for each word in every sentence, we\u2019ll\nplay a game of fill-in-the-blank.", "start": 520.56, "duration": 4.18}, {"text": "We\u2019ll train a model to encode up to that\nblank and then predict the word that should", "start": 524.74, "duration": 4.52}, {"text": "go there.", "start": 529.26, "duration": 1.1}, {"text": "And since we have the whole sentence, we know\nthe correct answer.", "start": 530.36, "duration": 3.61}, {"text": "First, we need to define the encoder.", "start": 533.97, "duration": 2.739}, {"text": "We need a model that can read in the input,\nwhich in this case is a sentence.", "start": 536.709, "duration": 4.711}, {"text": "To do this, we\u2019ll use a type of neural network\ncalled a Recurrent Neural Network or RNN.", "start": 541.42, "duration": 6.2}, {"text": "RNNs have a loop in them that lets them reuse\na single hidden layer, which gets updated", "start": 547.62, "duration": 4.89}, {"text": "as the model reads one word at a time.", "start": 552.51, "duration": 2.579}, {"text": "Slowly, the model builds up an understanding\nof the whole sentence, including which words", "start": 555.089, "duration": 4.381}, {"text": "came first or last, which words are modifying\nother words, and a whole bunch of other grammatical", "start": 559.47, "duration": 4.76}, {"text": "properties that are linked to meaning.", "start": 564.23, "duration": 2.09}, {"text": "Now, we can\u2019t just directly put words inside\na network.", "start": 566.32, "duration": 3.62}, {"text": "But we also don\u2019t have features we can easily\nmeasure and give the model either.", "start": 569.94, "duration": 3.99}, {"text": "Unlike images, we can\u2019t even measure pixel\nvalues.", "start": 573.93, "duration": 3.969}, {"text": "So we\u2019re going to ask the model to learn\nthe right representation for a word on its", "start": 577.899, "duration": 4.011}, {"text": "own (this is where the unsupervised learning\ncomes in).", "start": 581.91, "duration": 3.29}, {"text": "To do this, we\u2019ll start off by assigning\neach word a random representation -- in this", "start": 585.2, "duration": 4.64}, {"text": "case a random list of numbers called a vector.", "start": 589.84, "duration": 3.609}, {"text": "Next, our encoder will take in each of those\nrepresentations and combine them into a single", "start": 593.449, "duration": 4.651}, {"text": "/shared/ representation for the whole sentence.", "start": 598.1, "duration": 3.229}, {"text": "At this point, our representation might be\ngibberish, but in order to train the RNN,", "start": 601.329, "duration": 4.831}, {"text": "we need it to make predictions.", "start": 606.16, "duration": 1.9}, {"text": "For this particular problem, we\u2019ll consider\na very simple decoder, a single layer network", "start": 608.06, "duration": 5.449}, {"text": "that takes in the sentence representation\nvector, and then outputs a score for every", "start": 613.509, "duration": 4.52}, {"text": "possible word in our vocabulary.", "start": 618.029, "duration": 2.541}, {"text": "We can then interpret the highest scored word\nas our model\u2019s prediction.", "start": 620.57, "duration": 3.89}, {"text": "Then, we can use backpropagation to train\nthe RNN, like we\u2019ve done before with neural", "start": 624.46, "duration": 4.73}, {"text": "networks in Crash Course AI.", "start": 629.19, "duration": 1.72}, {"text": "So by training the model on which word to\npredict next, the model learn weights for", "start": 630.91, "duration": 4.549}, {"text": "the encoder RNN and the decoder prediction\nlayer.", "start": 635.459, "duration": 3.151}, {"text": "Plus, the model changes those random representations\nwe gave every word at the beginning.", "start": 638.61, "duration": 4.9}, {"text": "Specifically, if two words mean something\nsimilar, the model makes their vectors more", "start": 643.51, "duration": 4.7}, {"text": "similar.", "start": 648.21, "duration": 1.0}, {"text": "Using the vectors to help make a plot, we\ncan actually visualize word representations.", "start": 649.21, "duration": 4.66}, {"text": "For example, earlier we talked about chocolate\nand physics, so let\u2019s look at some word", "start": 653.87, "duration": 5.19}, {"text": "representations that researchers at Google\ntrained.", "start": 659.06, "duration": 2.94}, {"text": "Near \u201cchocolate,\u201d we have lots of foods\nlike cocoa and candy:", "start": 662.0, "duration": 4.16}, {"text": "By comparison, words with similar representations\nto \u201cphysics\u201d are newton and universe.", "start": 666.16, "duration": 5.42}, {"text": "This whole process has used unsupervised learning,\nand it\u2019s given us a basic way to learn some", "start": 671.58, "duration": 4.93}, {"text": "pretty interesting linguistic representations\nand word clusters.", "start": 676.51, "duration": 4.48}, {"text": "But taking in part of a sentence and predicting\nthe next word is just the tip of the iceberg", "start": 680.99, "duration": 4.709}, {"text": "for NLP.", "start": 685.699, "duration": 1.091}, {"text": "If our model took in English and produced\nSpanish, we\u2019d have a translation system.", "start": 686.79, "duration": 4.75}, {"text": "Or our model could read questions and produce answers, like Siri or Alexa try to do.", "start": 691.54, "duration": 5.77}, {"text": "Or our model could convert instructions into\nactions to control a household robot \u2026 Hey", "start": 697.31, "duration": 5.49}, {"text": "John Green Bot?", "start": 702.8, "duration": 5.77}, {"text": "Just kidding you\u2019re your own robot.", "start": 708.57, "duration": 2.11}, {"text": "Nobody controls you.", "start": 710.68, "duration": 1.349}, {"text": "But the representations of words that our\nmodel learns for one kind of task might not", "start": 712.029, "duration": 4.571}, {"text": "work for others.", "start": 716.6, "duration": 1.27}, {"text": "Like, for example, if we trained John-Green-bot\nbased on reading a bunch of cooking recipes,", "start": 717.87, "duration": 5.4}, {"text": "he might learn that roses are made of icing\nand placed on cakes.", "start": 723.27, "duration": 4.0}, {"text": "But he won\u2019t learn that cake roses are different\nfrom real roses that have thorns and make", "start": 727.27, "duration": 4.12}, {"text": "a pretty bouquet.", "start": 731.39, "duration": 1.259}, {"text": "Acquiring, encoding, and using written or\nspoken knowledge to help people is a huge", "start": 732.649, "duration": 4.791}, {"text": "and exciting task, because we use language\nfor so many things!", "start": 737.44, "duration": 4.37}, {"text": "Every time you type or talk to a computer,\nphone or other gadget, NLP is there.", "start": 741.81, "duration": 5.06}, {"text": "Now that we understand the basics, next week\nwe\u2019ll dive in and build a language model", "start": 746.87, "duration": 4.29}, {"text": "together in our second lab! See you then.", "start": 751.16, "duration": 4.4}, {"text": "Thank you to CuriosityStream for supporting PBS Digital Studios.", "start": 755.56, "duration": 3.94}, {"text": "CuriosityStream is a subscription streaming\nservice that offers documentaries and non\u00acfiction", "start": 759.5, "duration": 4.88}, {"text": "titles from a variety of filmmakers, including\nCuriosityStream originals.", "start": 764.38, "duration": 5.18}, {"text": "For example, you can stream Dream the Future\nin which host Sigourney Weaver asks the question,", "start": 769.56, "duration": 5.149}, {"text": "\u201cWhat will the future look like?\u201d as she\nexamines how new discoveries and research", "start": 774.709, "duration": 4.661}, {"text": "will impact our everyday lives in the year\n2050.", "start": 779.37, "duration": 3.399}, {"text": "You can learn more at curiositystream.com/crashcourse", "start": 782.769, "duration": 3.861}, {"text": "Or click the link in the description.", "start": 786.63, "duration": 2.06}, {"text": "Crash Course Ai is produced in association\nwith PBS Digital Studios!", "start": 788.69, "duration": 3.7}, {"text": "If you want to help keep Crash Course free\nfor everyone, forever, you can join our community", "start": 792.39, "duration": 4.639}, {"text": "on Patreon.", "start": 797.029, "duration": 1.711}, {"text": "And if you want to learn more about how human\nbrains process language, check out this episode", "start": 798.74, "duration": 4.26}, {"text": "of Crash Course Psychology.", "start": 803.0, "duration": 2.54}]