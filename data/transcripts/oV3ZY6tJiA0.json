[{"text": "Hi, I\u2019m Jabril, and welcome to CrashCourse\nAI!", "start": 4.08, "duration": 3.76}, {"text": "In the supervised learning episode, we taught\nJohn Green-bot to learn using a perceptron,", "start": 7.85, "duration": 4.13}, {"text": "a program that imitates one neuron.", "start": 11.98, "duration": 2.75}, {"text": "But our brains make decisions with 100 billion\nneurons, which have trillions of connections", "start": 14.73, "duration": 4.99}, {"text": "between them!", "start": 19.72, "duration": 1.0}, {"text": "We can actually do a lot more with AI if we\nconnect a bunch of perceptrons together, to", "start": 20.72, "duration": 4.399}, {"text": "create what\u2019s called an artificial neural\nnetwork.", "start": 25.119, "duration": 3.08}, {"text": "Neural networks are better than other methods\nfor certain tasks like, image recognition.", "start": 28.199, "duration": 4.231}, {"text": "The secret to their success is their hidden\nlayers, and they\u2019re mathematically very", "start": 32.43, "duration": 4.58}, {"text": "elegant.", "start": 37.01, "duration": 1.0}, {"text": "Both of these reasons are why neural networks\nare one of the most dominant machine learning", "start": 38.01, "duration": 3.22}, {"text": "technologies used today.", "start": 41.23, "duration": 2.03}, {"text": "[INTRO]", "start": 43.26, "duration": 8.9}, {"text": "Not that long ago, a big challenge in AI was\nreal-world image recognition, like recognizing", "start": 52.17, "duration": 4.93}, {"text": "a dog from a cat, and a car from a plane from\na boat.", "start": 57.1, "duration": 3.729}, {"text": "Even though we do it every day, it\u2019s really\nhard for computers.", "start": 60.829, "duration": 3.391}, {"text": "That\u2019s because computers are good at literal\ncomparisons, like matching 0s and 1s, one", "start": 64.22, "duration": 5.221}, {"text": "at a time.", "start": 69.441, "duration": 1.0}, {"text": "It\u2019s easy for a computer to tell that these\nimages are the same by matching the pixels.", "start": 70.441, "duration": 4.339}, {"text": "But before AI, a computer couldn\u2019t tell\nthat these images are of the same dog, and", "start": 74.78, "duration": 5.61}, {"text": "had no hope of telling that all of these different\nimages are dogs.", "start": 80.39, "duration": 3.67}, {"text": "So, a professor named Fei-Fei Li and a group\nof other machine learning and computer vision", "start": 84.06, "duration": 4.379}, {"text": "researchers wanted to help the research community\ndevelop AI that could recognize images.", "start": 88.439, "duration": 5.151}, {"text": "The first step was to create a huge public\ndataset of labeled real-world photos.", "start": 93.59, "duration": 5.059}, {"text": "That way, computer scientists around the world\ncould come up with and test different algorithms.", "start": 98.649, "duration": 4.441}, {"text": "They called this dataset ImageNet.", "start": 103.09, "duration": 2.29}, {"text": "It has 3.2 million labeled images, sorted\ninto 5,247 nested categories of nouns.", "start": 105.38, "duration": 7.589}, {"text": "Like for example, the \u201cdog\u201d label is nested\nunder \u201cdomestic animal,\u201d which is nested", "start": 112.969, "duration": 4.861}, {"text": "under \u201canimal.\u201d", "start": 117.83, "duration": 1.649}, {"text": "Humans are the best at reliably labeling data.", "start": 119.479, "duration": 3.331}, {"text": "But if one person did all this labeling, taking\n10 seconds per label, without any sleep or", "start": 122.81, "duration": 5.199}, {"text": "snack breaks, it would take them over a year!", "start": 128.009, "duration": 3.351}, {"text": "So ImageNet used crowd-sourcing and leveraged\nthe power of the Internet to cheaply spread", "start": 131.36, "duration": 5.2}, {"text": "the work between thousands of people.", "start": 136.56, "duration": 2.09}, {"text": "Once the data was in place, the researchers\nstarted an annual competition in 2010 to get", "start": 138.65, "duration": 4.83}, {"text": "people to contribute their best solutions\nto image recognition.", "start": 143.48, "duration": 3.41}, {"text": "Enter Alex Krizhevsky, who was a graduate\nstudent at the University of Toronto.", "start": 146.89, "duration": 4.73}, {"text": "In 2012, he decided to apply a neural network\nto ImageNet, even though similar solutions", "start": 151.62, "duration": 5.38}, {"text": "hadn\u2019t been successful in the past.", "start": 157.0, "duration": 1.73}, {"text": "His neural network, called AlexNet, had a\ncouple of innovations that set it apart.", "start": 158.73, "duration": 4.97}, {"text": "He used a lot of hidden layers, which we\u2019ll\nget to in a minute.", "start": 163.7, "duration": 3.44}, {"text": "He also used faster computation hardware to\nhandle all the math that neural networks do.", "start": 167.14, "duration": 4.51}, {"text": "AlexNet outperformed the next best approaches\nby over 10%.", "start": 171.65, "duration": 4.33}, {"text": "It only got 3 out of every 20 images wrong.", "start": 175.98, "duration": 2.679}, {"text": "In grade terms, it was getting a solid B while\nother techniques were scraping by with a low", "start": 178.659, "duration": 4.501}, {"text": "C.", "start": 183.16, "duration": 1.0}, {"text": "Since 2012, neural network solutions have\ntaken over the annual competition, and the", "start": 184.16, "duration": 3.62}, {"text": "results keep getting better and better.", "start": 187.78, "duration": 2.379}, {"text": "Plus, AlexNet sparked an explosion of research\ninto neural networks, which we started to", "start": 190.159, "duration": 4.401}, {"text": "apply to lots of things beyond image recognition.", "start": 194.56, "duration": 3.11}, {"text": "To understand how neural networks can be used\nfor these classification problems, we have", "start": 197.67, "duration": 4.38}, {"text": "to understand their architecture first.", "start": 202.05, "duration": 2.0}, {"text": "All neural networks are made up of an input\nlayer, an output layer, and any number of", "start": 204.05, "duration": 4.269}, {"text": "hidden layers in between.", "start": 208.319, "duration": 1.651}, {"text": "There are many different arrangements but\nwe\u2019ll use the classic multi-layer perceptron", "start": 209.97, "duration": 4.719}, {"text": "as an example.", "start": 214.689, "duration": 1.0}, {"text": "The input layer is where the neural network\nreceives data represented as numbers.", "start": 215.689, "duration": 4.58}, {"text": "Each input neuron represents a single feature,\nwhich is some characteristic of the data.", "start": 220.269, "duration": 5.371}, {"text": "Features are straightforward if you\u2019re talking\nabout something that\u2019s already a number,", "start": 225.64, "duration": 3.38}, {"text": "like grams of sugar in a donut.", "start": 229.02, "duration": 1.52}, {"text": "But, really, just about anything can be converted\nto a number.", "start": 230.54, "duration": 3.8}, {"text": "Sounds can be represented as the amplitudes\nof the sound wave.", "start": 234.34, "duration": 3.17}, {"text": "So each feature would have a number that represents\nthe amplitude at a moment in time.", "start": 237.51, "duration": 4.55}, {"text": "Words in a paragraph can be represented by\nhow many times each word appears.", "start": 242.06, "duration": 4.34}, {"text": "So each feature would have the frequency of\none word.", "start": 246.4, "duration": 3.28}, {"text": "Or, if we\u2019re trying to label an image of\na dog, each feature would represent information", "start": 249.68, "duration": 4.63}, {"text": "about a pixel.", "start": 254.31, "duration": 1.149}, {"text": "So for a grayscale image, each feature would\nhave a number representing how bright a pixel", "start": 255.459, "duration": 5.241}, {"text": "is.", "start": 260.7, "duration": 1.0}, {"text": "But for a color image, we can represent each\npixel with three numbers: the amount of red,", "start": 261.7, "duration": 5.33}, {"text": "green, and blue, which can be combined to\nmake any color on your computer screen.", "start": 267.03, "duration": 4.69}, {"text": "Once the features have data, each one sends\nits number to every neuron in the next layer,", "start": 271.72, "duration": 5.32}, {"text": "called the hidden layer.", "start": 277.04, "duration": 1.49}, {"text": "Then, each hidden layer neuron mathematically\ncombines all the numbers it gets.", "start": 278.53, "duration": 4.55}, {"text": "The goal is to measure whether the input data\nhas certain components.", "start": 283.08, "duration": 3.95}, {"text": "For an image recognition problem, these components\nmay be a certain color in the center, a curve", "start": 287.03, "duration": 5.22}, {"text": "near the top, or even whether the image contains\neyes, ears, or fur.", "start": 292.25, "duration": 4.6}, {"text": "Instead of answering yes or no, like the simple\nPerceptron from the previous episode, each", "start": 296.85, "duration": 4.51}, {"text": "neuron in the hidden layer does some slightly\nmore complicated math and outputs a number.", "start": 301.36, "duration": 5.52}, {"text": "And then, each neuron sends its number to\nevery neuron in the next layer, which could", "start": 306.88, "duration": 4.95}, {"text": "be another hidden layer or the output layer.", "start": 311.83, "duration": 2.9}, {"text": "The output layer is where the final hidden\nlayer outputs are mathematically combined", "start": 314.73, "duration": 4.66}, {"text": "to answer the problem.", "start": 319.39, "duration": 1.27}, {"text": "So, let\u2019s say we\u2019re just trying to label\nan image as a dog.", "start": 320.66, "duration": 3.17}, {"text": "We might have a single output neuron representing\na single answer - that the image is of a dog", "start": 323.83, "duration": 4.49}, {"text": "or not.", "start": 328.32, "duration": 1.0}, {"text": "But if there are many answers, like for example\nif we\u2019re labeling a bunch of images, we\u2019ll", "start": 329.32, "duration": 4.6}, {"text": "need a lot of output neurons.", "start": 333.92, "duration": 2.19}, {"text": "Each output neuron will correspond to the\nprobability for each label -- like for example,", "start": 336.11, "duration": 4.89}, {"text": "dog, car, spaghetti, and more.", "start": 341.0, "duration": 2.91}, {"text": "And then we can pick the answer with the highest\nprobability.", "start": 343.91, "duration": 3.06}, {"text": "The key to neural networks -- and really all\nof AI -- is math.", "start": 346.97, "duration": 4.01}, {"text": "And I get it.", "start": 350.98, "duration": 1.25}, {"text": "A neural network kind of seems like a black\nbox that does math and spits out an answer.", "start": 352.23, "duration": 4.69}, {"text": "I mean, those middle layers are even called\nhidden layers!", "start": 356.92, "duration": 2.99}, {"text": "But we can understand the gist of what\u2019s\nhappening by working through an example.", "start": 359.91, "duration": 4.08}, {"text": "Oh John Green Bot?", "start": 363.99, "duration": 2.72}, {"text": "Let\u2019s give John Green-bot a program with\na neural network that\u2019s been trained to", "start": 366.71, "duration": 6.43}, {"text": "recognize a dog in a grayscale photo.", "start": 373.14, "duration": 4.52}, {"text": "When we show him this photo first, every feature\nwill contain a number between 0 and 1 corresponding", "start": 383.68, "duration": 6.4}, {"text": "to the brightness of one pixel.", "start": 390.08, "duration": 2.63}, {"text": "And it\u2019ll pass this information to the hidden\nlayer.", "start": 392.71, "duration": 2.89}, {"text": "Now, let\u2019s focus on one hidden layer neuron.", "start": 395.6, "duration": 2.64}, {"text": "Since the neural network is already trained,\nthis neuron has a mathematical formula to", "start": 398.24, "duration": 4.28}, {"text": "look for a particular component in the image,\nlike a specific curve in the center.", "start": 402.52, "duration": 4.9}, {"text": "The curve at the top of the nose.", "start": 407.42, "duration": 1.45}, {"text": "If this neuron is focused on this specific\nshape and spot, it may not really care what\u2019s", "start": 408.87, "duration": 5.28}, {"text": "happening everywhere else.", "start": 414.15, "duration": 1.29}, {"text": "So it would multiply or weigh the pixel values\nfrom most of those features by 0 or close", "start": 415.44, "duration": 6.3}, {"text": "to 0.", "start": 421.74, "duration": 1.0}, {"text": "Because it\u2019s looking for bright pixels here,\nit would multiply these pixel values by a", "start": 422.74, "duration": 4.22}, {"text": "positive weight.", "start": 426.96, "duration": 1.17}, {"text": "But this curve is also defined by a darker\npart below.", "start": 428.13, "duration": 3.51}, {"text": "So the neuron would multiply these pixel values\nby a negative weight.", "start": 431.64, "duration": 4.14}, {"text": "This hidden neuron will add all the weighted\npixel values from the input neurons and squish", "start": 435.78, "duration": 4.44}, {"text": "the result so that it\u2019s between 0 and 1.", "start": 440.22, "duration": 2.7}, {"text": "The final number basically represents the\nguess of this neuron thinking that a specific", "start": 442.92, "duration": 4.49}, {"text": "curve, aka a dog nose, appeared in the image.", "start": 447.41, "duration": 3.42}, {"text": "Other hidden neurons are looking for other\ncomponents, like for example, a different", "start": 450.83, "duration": 4.25}, {"text": "curve in another part of the image , or a\nfuzzy texture.", "start": 455.08, "duration": 3.44}, {"text": "When all of these neurons pass their estimates\nonto the next hidden layer, those neurons", "start": 458.52, "duration": 4.34}, {"text": "may be trained to look for more complex components.", "start": 462.86, "duration": 3.39}, {"text": "Like, one hidden neuron may check whether\nthere\u2019s a shape that might be a dog nose.", "start": 466.25, "duration": 4.03}, {"text": "It probably doesn\u2019t care about data from\nprevious layers that looked for furry textures,", "start": 470.28, "duration": 4.69}, {"text": "so it weights those by 0 or close to 0.", "start": 474.97, "duration": 3.19}, {"text": "But it may really care about neurons that\nlooked for the \u201ctop of the nose\u201d and \u201cbottom", "start": 478.16, "duration": 4.36}, {"text": "of the nose\u201d and \u201cnostrils\u201d.", "start": 482.52, "duration": 1.56}, {"text": "It weights those by large positive numbers.", "start": 484.08, "duration": 2.63}, {"text": "Again, it would add up all the weighted values\nfrom the previous layer neurons, squish the", "start": 486.71, "duration": 4.49}, {"text": "value to be between 0 and 1, and pass this\nto the next layer.", "start": 491.2, "duration": 3.97}, {"text": "That\u2019s the gist of the math, but we\u2019re\nsimplifying a bit.", "start": 495.17, "duration": 4.07}, {"text": "It\u2019s important to know that neural networks\ndon\u2019t actually understand ideas like \u201cnose\u201d", "start": 499.24, "duration": 4.67}, {"text": "or \u201ceyelid.\u201d", "start": 503.91, "duration": 1.06}, {"text": "Each neuron is doing a calculation on the\ndata it\u2019s given and just flagging specific", "start": 504.97, "duration": 4.74}, {"text": "patterns of light and dark.", "start": 509.71, "duration": 1.87}, {"text": "After a few more hidden layers, we reach the\noutput layer with one neuron!", "start": 511.58, "duration": 4.95}, {"text": "So after one more weighted addition of the\nprevious layer\u2019s data, which happens in", "start": 516.53, "duration": 4.73}, {"text": "the output neuron, the network should have\na good estimate if this image is a dog.", "start": 521.26, "duration": 4.89}, {"text": "Which means, John Green-bot should have a\ndecision.", "start": 526.15, "duration": 2.79}, {"text": "John Green-bot: Output neuron value: 0.93.", "start": 529.04, "duration": 4.66}, {"text": "Probability that this is a dog: 93%!", "start": 533.7, "duration": 4.18}, {"text": "Hey John Green Bot nice job!", "start": 537.88, "duration": 3.52}, {"text": "Thinking about how a neural network would\nprocess just one image makes it clearer why", "start": 545.42, "duration": 4.7}, {"text": "AI needs fast computers.", "start": 550.12, "duration": 1.96}, {"text": "Like I mentioned before, each pixel in a color\nimage will be represented by 3 numbers --- how", "start": 552.08, "duration": 5.37}, {"text": "much red, green, and blue it has.", "start": 557.45, "duration": 2.66}, {"text": "So to process a 1000 by 1000 pixel image,\nwhich in comparison is a small 3 by 3 inch", "start": 560.11, "duration": 5.78}, {"text": "photo, a neural network needs to look at 3\nmillion features!", "start": 565.89, "duration": 4.13}, {"text": "AlexNet needed more than 60 million neurons\nto achieve this, which is a ton of math and", "start": 570.02, "duration": 5.38}, {"text": "could take a lot of time to compute.", "start": 575.4, "duration": 2.37}, {"text": "Which is something we should keep in mind\nwhen designing neural networks to solve problems.", "start": 577.77, "duration": 4.23}, {"text": "People are really excited about using deeper\nneural networks, which are networks with more", "start": 582.0, "duration": 5.24}, {"text": "hidden layers, to do deep learning.", "start": 587.24, "duration": 2.46}, {"text": "Deep networks can combine input data in more\ncomplex ways to look for more complex components,", "start": 589.7, "duration": 6.11}, {"text": "and solve trickier problems.", "start": 595.81, "duration": 1.83}, {"text": "But we can\u2019t make all networks like a billion\nlayers deep, because more hidden layers means", "start": 597.64, "duration": 5.11}, {"text": "more math which again would mean that we need\nfaster computers.", "start": 602.75, "duration": 4.17}, {"text": "Plus, as a network get deeper, it gets harder\nfor us to make sense of why it\u2019s giving", "start": 606.92, "duration": 4.82}, {"text": "the answers it does.", "start": 611.74, "duration": 1.35}, {"text": "Each neuron in the first hidden layer is looking\nfor some specific component of the input data.", "start": 613.09, "duration": 5.48}, {"text": "But in deeper layers, those components get\nmore abstract from how humans would describe", "start": 618.57, "duration": 4.21}, {"text": "the same data.", "start": 622.78, "duration": 1.37}, {"text": "Now, this may not seem like a big deal, but\nif a neural network was used to deny our loan", "start": 624.15, "duration": 4.82}, {"text": "request for example, we\u2019d want to know why.", "start": 628.97, "duration": 3.58}, {"text": "Which features made the difference?", "start": 632.55, "duration": 2.51}, {"text": "How were they weighed towards the final answer?", "start": 635.06, "duration": 2.09}, {"text": "In many countries, we have the legal right\nto understand why these kinds of decisions", "start": 637.15, "duration": 4.28}, {"text": "were made.", "start": 641.43, "duration": 1.0}, {"text": "And neural networks are being used to make\nmore and more decisions about our lives.", "start": 642.43, "duration": 4.41}, {"text": "Most banks for example use neural networks\nto detect and prevent fraud.", "start": 646.84, "duration": 3.41}, {"text": "Many cancer tests, like the Pap test for cervical\ncancer, use a neural network to look at an", "start": 650.25, "duration": 4.93}, {"text": "image of cells under a microscope, and decide\nwhether there\u2019s a risk of cancer.", "start": 655.18, "duration": 4.18}, {"text": "And neural networks are how Alexa understands\nwhat song you\u2019re asking her to play and", "start": 659.36, "duration": 3.68}, {"text": "how Facebook suggests tags for our photos.", "start": 663.04, "duration": 2.54}, {"text": "Understanding how all this happens is really\nimportant to being a human in the world right", "start": 665.58, "duration": 4.08}, {"text": "now, whether or not you want to build your\nown neural network.", "start": 669.67, "duration": 2.93}, {"text": "So this was a lot of big-picture stuff, but\nthe program we gave John Green-bot had already", "start": 672.6, "duration": 5.22}, {"text": "been trained to recognize dogs.", "start": 677.82, "duration": 2.33}, {"text": "The neurons already had algorithms that weighted\ninputs.", "start": 680.15, "duration": 3.74}, {"text": "Next time, we\u2019ll talk about the learning\nprocess used by neural networks to get to", "start": 683.89, "duration": 3.831}, {"text": "the right weights for every neuron, and why\nthey need so much data to work well.", "start": 687.721, "duration": 5.759}, {"text": "Crash Course Ai is produced in association with PBS Digital Studios.", "start": 722.8, "duration": 4.44}, {"text": "If you want to help keep all Crash Course\nfree for everyone, forever, you can join our", "start": 727.24, "duration": 3.37}, {"text": "community on Patreon.", "start": 730.61, "duration": 2.1}, {"text": "And if you want to learn more about the math\nbehind neural networks, check out this video", "start": 732.71, "duration": 3.44}, {"text": "from Crash Course Statistics about them.", "start": 736.15, "duration": 3.35}]