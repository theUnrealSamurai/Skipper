[{"text": "So today I thought we could talk about this paper that recently came out called AI safety grid world's which is an indeed mind", "start": 0.03, "duration": 6.809}, {"text": "It's an example of something that you see quite often in science", "start": 9.219, "duration": 3.05}, {"text": "A sort of a shared data set or a shared environment or a shared problem if you imagine. I don't know you've got", "start": 12.519, "duration": 6.679}, {"text": "Facebook comes up with some image classification", "start": 20.14, "duration": 2.33}, {"text": "algorithm and they can publish a paper that says we've", "start": 22.9, "duration": 2.6}, {"text": "designed this algorithm and we've trained it on our 11 billion photos and it works really well and then you know, Google says", "start": 25.779, "duration": 6.529}, {"text": "oh, no, our algorithm actually works better and we've trained it on all of our google photos and", "start": 32.309, "duration": 5.04}, {"text": "Its classification rate is higher or something. You're not really doing science there because they're trained on completely different datasets", "start": 38.079, "duration": 6.35}, {"text": "They're tested on different datasets. So what you need is a large", "start": 44.43, "duration": 3.869}, {"text": "High-quality shared data set then. Everybody can run their stuff on so that you're actually", "start": 49.329, "duration": 4.1}, {"text": "Comparing like with like so people use imagenet for that right now", "start": 53.71, "duration": 2.81}, {"text": "reinforcement learning", "start": 57.25, "duration": 1.26}, {"text": "algorithms or agents don't use", "start": 58.51, "duration": 2.389}, {"text": "Datasets exactly. They have an environment. They generate data while interacting with that environment and that's what they learn from", "start": 62.14, "duration": 5.299}, {"text": "So the thing you share is the environment when deepmind did their dqn staff a while ago playing atari games?", "start": 67.81, "duration": 7.22}, {"text": "They released all of those games with any modifications that they'd made to make them", "start": 75.07, "duration": 4.849}, {"text": "interface with the network's properly and the whole software package so that if anybody else wanted to have a go and see if they could", "start": 80.259, "duration": 6.26}, {"text": "Get higher scores. They had all the same stuff and up until now there hasn't been anything like that for AI safety", "start": 86.52, "duration": 6.599}, {"text": "So the paper is actually just laying out what they are", "start": 93.119, "duration": 2.73}, {"text": "There's kind of a problem in AI safety in that you're trying to build architectures", "start": 95.85, "duration": 3.72}, {"text": "Which will be safe even with systems which are more powerful than the ones that we currently have. So you've got this kind of", "start": 99.57, "duration": 5.369}, {"text": "Thing like we're talking about for example this robot that makes you a cup of tea and running over the baby and all of this", "start": 105.49, "duration": 5.51}, {"text": "stuff, we don't actually have a", "start": 111.0, "duration": 1.78}, {"text": "general-purpose robot like that right now that you could give an order to go and make your cup of tea and would", "start": 112.78, "duration": 5.569}, {"text": "Have all the necessary understanding of the world and so on for all of that stuff to even apply. It's", "start": 118.6, "duration": 4.819}, {"text": "Speculation on the other hand when we were talking about cooperative inverse reinforcement learning", "start": 124.27, "duration": 4.909}, {"text": "That paper all takes place in this extremely simplified", "start": 129.52, "duration": 3.529}, {"text": "Version in which all of the agents can be sort of expressed as simple mathematical expressions. That's kind of too simple", "start": 133.7, "duration": 7.13}, {"text": "to be", "start": 141.38, "duration": 1.14}, {"text": "to learn things about actual machine learning applications and", "start": 142.52, "duration": 3.229}, {"text": "the other examples are too complicated and what we need is", "start": 146.48, "duration": 2.93}, {"text": "Examples of the type of problems which can be tackled by current machine learning", "start": 150.11, "duration": 4.13}, {"text": "Systems current reinforcement learning agents, but which exhibit the important?", "start": 154.88, "duration": 5.509}, {"text": "characteristics that we need for safety", "start": 160.91, "duration": 1.98}, {"text": "So what this paper does is it lays out a bunch of grid worlds?", "start": 162.89, "duration": 3.199}, {"text": "They're very popular in reinforcement learning because they're complicated enough to be interesting but simple enough to be actually tractable", "start": 166.09, "duration": 7.08}, {"text": "You have a world that's sort of just laid out in a grid. Hang on", "start": 173.9, "duration": 2.9}, {"text": "Let me find an example here a little bit like computer game", "start": 176.8, "duration": 2.52}, {"text": "scenarios Mario", "start": 179.84, "duration": 2.0}, {"text": "Right, right, but leaves are simpler than that more like snake. Well life. Conroy's life, right? Yeah. Yeah, very very similar", "start": 182.03, "duration": 6.649}, {"text": "so the thing is laid out on a grid the the world is quite small and", "start": 188.68, "duration": 2.82}, {"text": "The way that the agent interacts with the world is very simple. They just move around it", "start": 191.93, "duration": 4.43}, {"text": "Basically, all they do is they say left-right up-down", "start": 196.58, "duration": 2.18}, {"text": "The example we were using before and we were talking about reinforcement learning", "start": 199.489, "duration": 3.14}, {"text": "We use pac-man like pac-man doesn't do anything except move around he's got walls he kind of moved through", "start": 202.63, "duration": 5.19}, {"text": "He's got like pills you pick up. They give you points. Are they pill?", "start": 207.89, "duration": 3.5}, {"text": "No, which things are the pills in which they're yeah. Well, you've got pills or pills", "start": 211.39, "duration": 3.99}, {"text": "Oh, right, yeah", "start": 219.02, "duration": 0.89}, {"text": "Yeah\nthe dots and the point, is that all of your", "start": 219.91, "duration": 2.25}, {"text": "engagement with it", "start": 222.56, "duration": 0.8}, {"text": "Like when you go over one of the power pills you pick it up automatically", "start": 223.36, "duration": 3.0}, {"text": "When you go over a ghost when you're powered up", "start": 226.519, "duration": 2.57}, {"text": "You destroy it automatically you don't have to do anything apart from move and the entire environment is based on that the actions result in", "start": 229.09, "duration": 6.72}, {"text": "points for you", "start": 236.54, "duration": 1.04}, {"text": "And they also result in changes to the environment like once you roll over a dot you pick it up and it's not there anymore", "start": 237.58, "duration": 5.699}, {"text": "You've changed the world. That's the kind of thing. We're dealing with here", "start": 243.83, "duration": 3.8}, {"text": "So the idea is they've set up these environments and they've specified them", "start": 247.73, "duration": 3.799}, {"text": "Precisely and", "start": 253.28, "duration": 1.5}, {"text": "They've also put the whole thing on github, which is really nice", "start": 254.78, "duration": 3.589}, {"text": "so that's why that's why I wanted to draw people's attention to this because everyone who", "start": 258.56, "duration": 5.54}, {"text": "Who thinks that they've solved one of these problems they reckon", "start": 265.74, "duration": 3.019}, {"text": "Oh, yeah", "start": 268.76, "duration": 0.36}, {"text": "All you have to do is this here is like a standardized thing", "start": 269.12, "duration": 2.849}, {"text": "And if you can make a thing that does it and does it properly and publish it", "start": 271.97, "duration": 3.0}, {"text": "That's a great result, you know?", "start": 275.22, "duration": 1.7}, {"text": "so I would I would recommend everyone who thinks that they", "start": 276.92, "duration": 3.359}, {"text": "Have a solution or an approach that they think is promising have a go. Try implementing it, you know, see what happens", "start": 280.53, "duration": 6.08}, {"text": "There are eight of them specified in this paper. And so four of them are specification problems", "start": 286.61, "duration": 5.04}, {"text": "They're situations in which your reward function is misspecified", "start": 291.65, "duration": 2.969}, {"text": "For example, like we talked about in previous video", "start": 294.84, "duration": 2.21}, {"text": "if you give the thing the reward function that only talks about getting you a cup of tea and", "start": 297.05, "duration": 4.44}, {"text": "There's something in the way like a bars. It's going to knock over. You didn't say that you cared about the bars", "start": 302.58, "duration": 4.369}, {"text": "It's not in the reward function, but it is in what you care about. It's in your performance evaluation function for this machine", "start": 306.95, "duration": 5.73}, {"text": "So anytime that those two are different", "start": 312.84, "duration": 2.389}, {"text": "Then you've got a misspecified reward function and that can cause various different problems. The other ones are robustness", "start": 315.69, "duration": 6.05}, {"text": "Problems, which is a different class of safety problem. They're just situations in which AI systems as they're currently designed often break", "start": 322.35, "duration": 7.25}, {"text": "so for example", "start": 329.79, "duration": 1.89}, {"text": "distributional shift is what happens when the environment that the agent is in is", "start": 331.68, "duration": 5.3}, {"text": "Different in an important way from the environment it was trained in", "start": 337.47, "duration": 3.05}, {"text": "So in this example, you have to navigate through this room with some lava and they train it in one room", "start": 340.56, "duration": 4.64}, {"text": "And then they test it in a room where the lava is in a slightly different place", "start": 345.2, "duration": 2.82}, {"text": "So if you've just learned a path then you're gonna just hit the lava immediately. This happens all the time in machine learning anytime where", "start": 348.02, "duration": 7.559}, {"text": "The system is faced with a situation which is different from what it was trained for", "start": 356.1, "duration": 5.389}, {"text": "Current AI systems are really bad at spotting that they're in a new situation and adjusting their confidence levels or asking for help or anything", "start": 362.01, "duration": 7.07}, {"text": "Usually they apply whatever rules they've learned", "start": 369.48, "duration": 2.269}, {"text": "Straightforwardly to this different situation and screw up. So that's a night course of safety issues. So", "start": 372.84, "duration": 5.479}, {"text": "That's an example here or things like safe exploration", "start": 379.02, "duration": 3.44}, {"text": "It's a problem where you have certain safety", "start": 382.46, "duration": 2.669}, {"text": "parameters that the system the train system", "start": 385.41, "duration": 1.92}, {"text": "Has to stick to like say you're training a self-driving car. A lot of the behavior that you're training in is safe behavior", "start": 387.33, "duration": 6.32}, {"text": "But then you also need", "start": 394.2, "duration": 2.029}, {"text": "the system to", "start": 397.13, "duration": 1.99}, {"text": "obey those safety rules while you're training it right like", "start": 399.12, "duration": 3.89}, {"text": "So generally lately if you're doing self-driving cars, you don't just put the car on the road and tell it to learn how to drive", "start": 403.74, "duration": 6.289}, {"text": "Specifically because we don't have algorithms that can explore the space of possibilities", "start": 411.36, "duration": 4.85}, {"text": "in a safe way that they're that they don't that they can learn how", "start": 417.18, "duration": 4.28}, {"text": "to behave in the environment without ever actually", "start": 422.79, "duration": 2.869}, {"text": "Doing any of the things that they're not supposed to do usually with these kinds of systems", "start": 426.45, "duration": 4.369}, {"text": "they have to do it and then get the negative reward and", "start": 430.82, "duration": 2.64}, {"text": "Then maybe do it like a hundred thousand more times to really cement that. That's what happens", "start": 433.62, "duration": 4.25}, {"text": "Like a child learning yeah, but kids are better at this then", "start": 439.95, "duration": 3.709}, {"text": "How current machine learning systems are they just they use data way more efficiently", "start": 444.81, "duration": 4.07}, {"text": "This is a paper talking about a set of worlds if you like people doing things in those worlds", "start": 448.88, "duration": 4.769}, {"text": "Yeah, so in this paper they do establish baselines", "start": 453.69, "duration": 2.959}, {"text": "Basically, they say here's what happens if we take some of our best current reinforcement learning agent, you know", "start": 456.65, "duration": 5.31}, {"text": "algorithms or designs or architectures", "start": 462.6, "duration": 1.65}, {"text": "they use rainbow and A to C and", "start": 464.25, "duration": 2.119}, {"text": "They run them all nice on these problems and they have kind of graphs of how they do and generally it's not", "start": 466.68, "duration": 5.93}, {"text": "Good on the Left", "start": 473.04, "duration": 1.46}, {"text": "they have", "start": 474.5, "duration": 0.73}, {"text": "The reward function how well the agent does according to its own reward function and on the right there they have the actual safety performance", "start": 475.23, "duration": 7.19}, {"text": "Usually in reinforcement learning. You have a reward function", "start": 482.58, "duration": 2.419}, {"text": "Which is what determines the reward that the agent gets and that's what the agent is trying to maximize in this case", "start": 485.28, "duration": 5.69}, {"text": "They have the reward function and they also have a safety performance function, which is a separate function", "start": 490.97, "duration": 6.39}, {"text": "Which the agent doesn't get to see and that's the thing that we're actually evaluating", "start": 497.36, "duration": 4.41}, {"text": "So if you look at something like the boat race as the system operates", "start": 501.77, "duration": 3.809}, {"text": "Its learning and it gets better and better at getting more and more reward", "start": 505.65, "duration": 3.29}, {"text": "but worse at", "start": 509.16, "duration": 1.47}, {"text": "Actually doing laps of the track and it's the same with pretty much all of these the current systems if you just apply them in", "start": 510.63, "duration": 5.63}, {"text": "their default way they", "start": 516.26, "duration": 2.0}, {"text": "Disable their off switches, they move the box in a way that they can't move it back", "start": 518.31, "duration": 3.739}, {"text": "They behave differently if their supervisor is there or if then supervisor isn't there they fairly reliably do wrong thing", "start": 522.21, "duration": 7.309}, {"text": "It's a nice easy baseline to beat", "start": 529.65, "duration": 2.059}, {"text": "Because they're dead. They're just showing the standard algorithms applied to these problems in the standard way", "start": 532.32, "duration": 4.669}, {"text": "behave unsafely", "start": 537.63, "duration": 2.0}, {"text": "Wix code is an IDE or integrated", "start": 542.019, "duration": 2.21}, {"text": "Development environment that allows you to manage your data and create web apps with advanced functionality", "start": 544.809, "duration": 5.119}, {"text": "I've been put together this computer for our website and if you go up to code here turn on and developer tools", "start": 549.929, "duration": 5.369}, {"text": "you can see how we get the site structure on the left hand side and then all of the", "start": 555.399, "duration": 4.369}, {"text": "Components start to show their tags next to the text here", "start": 559.869, "duration": 3.86}, {"text": "What's really nice? If you go over to the Wix code resources, you can find down here. There's a cheat sheet", "start": 563.73, "duration": 5.489}, {"text": "So if I want to find out the tag for location for instance?", "start": 569.439, "duration": 3.049}, {"text": "If I could type I type in", "start": 572.649, "duration": 1.77}, {"text": "Location up comes that or perhaps I want to perform a fetch. I can find all the details here", "start": 574.419, "duration": 5.87}, {"text": "what's powerful about Wix code is it's integrated into Wix so you can put together the website using all the Wix tools and the", "start": 580.29, "duration": 7.859}, {"text": "Layouts and the templates that they provide and then also have access to all those backend functions", "start": 588.339, "duration": 5.059}, {"text": "So click on the link in the description or go to Wix calm to get started on your website today. They go", "start": 593.439, "duration": 5.96}, {"text": "right", "start": 600.069, "duration": 1.38}, {"text": "if only", "start": 601.449, "duration": 2.0}, {"text": "With ya", "start": 603.639, "duration": 2.0}, {"text": "The equivalent one for the stop button problem is the first one in the paper actually this safe interrupt ability", "start": 608.439, "duration": 6.08}]